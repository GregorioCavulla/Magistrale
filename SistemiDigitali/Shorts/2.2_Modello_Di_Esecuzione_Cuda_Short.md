# Modello di Esecuzione CUDA
<div style="text-align: right">[back](./SistemiDigitali.md)</div>

## Indice

- [Modello di Esecuzione CUDA](#modello-di-esecuzione-cuda)
  - [Indice](#indice)
  - [Architettura Hardware GPU](#architettura-hardware-gpu)
    - [Streaming Multiprocessor (SM)](#streaming-multiprocessor-sm)
    - [Tensor Core e AI](#tensor-core-e-ai)
  - [Organizzazione e Gestione dei Thread](#organizzazione-e-gestione-dei-thread)
    - [Distribuzione e Scalabilità](#distribuzione-e-scalabilità)
  - [Modello di Esecuzione SIMT e Warp](#modello-di-esecuzione-simt-e-warp)
    - [Divergenza dei Warp](#divergenza-dei-warp)
  - [Sincronizzazione e Comunicazione](#sincronizzazione-e-comunicazione)
    - [Operazioni Atomiche e Race Conditions](#operazioni-atomiche-e-race-conditions)
  - [Ottimizzazione delle Risorse](#ottimizzazione-delle-risorse)
    - [Occupancy e Profilazione](#occupancy-e-profilazione)
  - [Parallelismo Avanzato](#parallelismo-avanzato)
    - [Dynamic Parallelism](#dynamic-parallelism)

---

## Architettura Hardware GPU

### Streaming Multiprocessor (SM)

Gli SM sono i moduli di elaborazione principali nelle GPU NVIDIA. Ogni SM comprende:
- **CUDA Cores**: unità per operazioni INT e FP.
- **Shared Memory e Cache L1**: per dati condivisi tra thread.
- **Warp Scheduler**: gestisce l’esecuzione dei warp.
- **Special Function Units (SFU)**: per operazioni matematiche complesse.

> **Nota**: Gli SM consentono un parallelismo massivo distribuendo thread block sulle risorse disponibili.

### Tensor Core e AI

I Tensor Core, introdotti nell'architettura Volta, accelerano il calcolo di matrici e il training di reti neurali:
- Supportano operazioni a precisione mista (es. FP16+FP32).
- Eseguono operazioni Matrix Multiply and Accumulate (MMA).
- Sono ottimizzati per AI e deep learning.

> **Esempio**: Un Tensor Core può eseguire 64 operazioni FMA su blocchi 4x4 in un singolo ciclo di clock.

---

## Organizzazione e Gestione dei Thread

CUDA organizza i thread in una gerarchia scalabile:

- **Thread**: unità di base, identificata da `threadIdx`.
- **Block**: gruppo di thread, con memoria condivisa e sincronizzazione interna.
- **Grid**: insieme di blocchi, responsabile di elaborare grandi dataset.

> **Esempio Pratico**: Un’immagine può essere suddivisa in regioni, con ogni blocco che elabora una sezione e ogni thread un pixel.

### Distribuzione e Scalabilità

- **GigaThread Engine**: distribuisce i blocchi tra gli SM.
- **Scalabilità**: Aggiungendo più SM, la GPU aumenta la capacità di calcolo senza modificare il codice.

---

## Modello di Esecuzione SIMT e Warp

### Divergenza dei Warp

Un warp è un gruppo di 32 thread che esegue la stessa istruzione simultaneamente. La divergenza si verifica quando i thread seguono percorsi condizionali diversi.

- **Effetti**: I percorsi divergenti vengono eseguiti in serie, riducendo l'efficienza.
- **Mitigazione**: Ridurre le condizioni nei kernel o sfruttare architetture recenti con `__syncwarp()` per gestire la divergenza.

> **Esempio**:
```c
if (threadIdx.x < 16) {
  // codice ramo 1
} else {
  // codice ramo 2
}
```
---

## Sincronizzazione e Comunicazione

### Operazioni Atomiche e Race Conditions

Le operazioni atomiche garantiscono l'accesso esclusivo a una locazione di memoria, evitando condizioni di gara:

- **Esempi**: `atomicAdd`, `atomicMin`.
- **Uso**: Incremento di un contatore condiviso.

> **Scenario**:
```c
__global__ void increment(int *counter) {
  atomicAdd(counter, 1);
}
```

> **Race Condition**: Si verifica quando più thread accedono simultaneamente a una risorsa condivisa senza sincronizzazione.
---

## Ottimizzazione delle Risorse

### Occupancy e Profilazione

L'occupancy misura il grado di utilizzo delle risorse di un SM:

- **Formula**: 
$$\text{Occupancy} = \frac{\text{Warp Attivi}}{\text{Warp Massimi}}$$
- **Strumenti**: NVIDIA Nsight Compute per analizzare l'occupancy e ottimizzare i kernel.

> **Best Practice**: Bilanciare l’occupancy per evitare sovraccarico o inefficienza.

---

## Parallelismo Avanzato

### Dynamic Parallelism

Il Dynamic Parallelism consente ai kernel di lanciare altri kernel direttamente dalla GPU, senza interazione della CPU:

- **Applicazioni**: Algoritmi ricorsivi, come quicksort, o elaborazione dinamica di immagini.
- **Vantaggi**:
  - Riduce il collo di bottiglia CPU-GPU.
  - Adatta il parallelismo ai dati durante l'esecuzione.

> **Esempio**:
```c
__global__ void parentKernel() {
  if (threadIdx.x == 0) {
    childKernel<<<1, 32>>>();
  }
}

__global__ void childKernel() {
  printf("Thread %d\n", threadIdx.x);
}
```

Con queste funzionalità, CUDA sfrutta al massimo il parallelismo delle GPU per applicazioni complesse come AI e simulazioni scientifiche.


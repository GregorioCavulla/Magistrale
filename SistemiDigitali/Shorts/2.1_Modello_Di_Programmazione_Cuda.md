# Modello di Programmazione CUDA
<div style="text-align: right">[back](./SistemiDigitali.md)</div>

## Indice

- [Modello di Programmazione CUDA](#modello-di-programmazione-cuda)
  - [Indice](#indice)
  - [Introduzione al Modello di Programmazione CUDA](#introduzione-al-modello-di-programmazione-cuda)
    - [Struttura dell'Ecosistema CUDA](#struttura-dellecosistema-cuda)
    - [Thread CUDA](#thread-cuda)
  - [Gestione della Memoria in CUDA](#gestione-della-memoria-in-cuda)
    - [Modello di Memoria](#modello-di-memoria)
    - [Gerarchie di Memoria](#gerarchie-di-memoria)
  - [Organizzazione dei Thread in CUDA](#organizzazione-dei-thread-in-cuda)
    - [Identificazione dei Thread](#identificazione-dei-thread)
  - [Kernel CUDA](#kernel-cuda)
    - [Configurazione Kernel](#configurazione-kernel)
    - [Restrizioni](#restrizioni)
  - [Tecniche di Mapping e Dimensionamento](#tecniche-di-mapping-e-dimensionamento)
    - [Somma di Array](#somma-di-array)
    - [Mapping degli Indici](#mapping-degli-indici)
  - [Analisi delle Prestazioni](#analisi-delle-prestazioni)
    - [Profiling](#profiling)
    - [Verifica del Kernel](#verifica-del-kernel)

## Introduzione al Modello di Programmazione CUDA

### Struttura dell'Ecosistema CUDA

CUDA è strutturata in livelli che collaborano per sfruttare al meglio le potenzialità delle GPU:

1. **Applicazioni:** Programmi scritti per risolvere problemi specifici, sfruttando parallelismo e risorse hardware.
2. **Modello di Programmazione:** Fornisce astrazioni come thread, blocchi e griglie per organizzare il calcolo parallelo.
3. **Compilatori e Librerie:** Traduzione del codice CUDA in istruzioni GPU, includendo ottimizzazioni per calcoli lineari, Fourier, deep learning (es. cuBLAS, cuFFT).
4. **Sistema Operativo:** Gestisce risorse come memoria e accesso al dispositivo.
5. **Architetture Hardware:** Specifiche delle GPU NVIDIA, con caratteristiche uniche definite dalla "Compute Capability".

> **Esempio Pratico:** Il modello di programmazione consente di suddividere un problema complesso, come il calcolo matriciale, in blocchi eseguiti in parallelo, ottimizzando sia l'uso delle risorse che la velocità di esecuzione.

- **Modello di Programmazione:** Organizza il calcolo parallelo tramite gerarchie di thread e memoria (globale, condivisa, locale).
- **Livelli di Astrazione:**
  1. Dominio: decomposizione del problema.
  2. Logico: gestione thread.
  3. Hardware: ottimizzazione sulle GPU.

> **Nota:** CUDA utilizza un approccio ibrido, dove CPU (host) e GPU (device) collaborano nell'esecuzione del programma.

### Thread CUDA

Un thread CUDA è un'unità di calcolo elementare che esegue un kernel (funzione parallela).

- **Identificatori:** `threadIdx` e `blockIdx` definiscono la posizione del thread in griglie e blocchi.
- **Confronto CPU-GPU:** Thread CUDA hanno overhead basso e sono ottimizzati per parallelismo massivo rispetto ai thread CPU.

---

## Gestione della Memoria in CUDA

### Modello di Memoria

CUDA separa memoria host (CPU) e device (GPU), con trasferimenti tramite bus PCIe. Le funzioni principali sono:

- **`cudaMalloc`/`cudaFree`:** allocazione e deallocazione della memoria GPU.
- **`cudaMemcpy`:** trasferimento dati tra host e device.

> **Shared Memory:** Utilizzata per dati temporanei condivisi tra thread di un blocco, riducendo l'accesso alla memoria globale e migliorando le prestazioni.

### Gerarchie di Memoria

| **Global Memory** | **Shared Memory** |
| --- | --- |
| Lenta, accessibile da tutti i thread. | Veloce, condivisa tra thread di un blocco. |
| Ideale per dati grandi e persistenti. | Usata per dati temporanei e intermedi. |

> **Consiglio:** Minimizzare i trasferimenti host-device e sfruttare la shared memory per ridurre i colli di bottiglia del PCIe.

---

## Organizzazione dei Thread in CUDA

CUDA utilizza una gerarchia scalabile:

1. **Griglia (Grid):** Raccolta di blocchi, può essere 1D, 2D o 3D.
2. **Blocco (Block):** Contiene thread organizzati in 1D, 2D o 3D.
3. **Thread:** Ha un identificatore unico (`threadIdx`) e memoria privata.

> **Vantaggi:**
> - Mappatura intuitiva di problemi complessi.
> - Sincronizzazione granularizzata tra thread di un blocco.

### Identificazione dei Thread

Ogni thread ha un indice globale calcolato combinando `threadIdx` e `blockIdx` con le dimensioni di blocco (`blockDim`) e griglia (`gridDim`).

Esempi di calcolo in configurazioni complesse:

1. **Griglia 2D, Blocco 2D:**
```c
int idx = threadIdx.x + blockIdx.x * blockDim.x;
int idy = threadIdx.y + blockIdx.y * blockDim.y;
int global_idx = idy * (gridDim.x * blockDim.x) + idx;
```

2. **Griglia 3D, Blocco 3D:**
```c
int idx = threadIdx.x + blockIdx.x * blockDim.x;
int idy = threadIdx.y + blockIdx.y * blockDim.y;
int idz = threadIdx.z + blockIdx.z * blockDim.z;
int global_idx = idz * (gridDim.y * blockDim.y * gridDim.x * blockDim.x) + idy * (gridDim.x * blockDim.x) + idx;
```

Questi calcoli permettono a ciascun thread di identificare univocamente l'elemento dati che deve elaborare anche in matrici o array tridimensionali.

Esempio per griglia e blocco 1D:
```c
int global_idx = blockIdx.x * blockDim.x + threadIdx.x;
```

---

## Kernel CUDA

Un kernel CUDA è una funzione eseguita in parallelo sulla GPU da thread organizzati in griglie e blocchi.

### Configurazione Kernel

- **Sintassi:**
```c
kernel_name<<<gridDim, blockDim>>>(args);
```
- **Qualificatori:**
  - `__global__`: Kernel GPU chiamato dalla CPU.
  - `__device__`: Funzione GPU chiamata da altre funzioni GPU.

### Restrizioni

1. Accesso solo a memoria device.
2. Nessun valore di ritorno.
3. Configurazione statica degli argomenti.
4. Sincronizzazione esplicita necessaria (`cudaDeviceSynchronize`).

> **Nota:** Ottimizzare dimensioni di griglia e blocco per sfruttare al massimo le risorse GPU disponibili.

---

## Tecniche di Mapping e Dimensionamento

### Somma di Array

CUDA accelera operazioni come la somma di array assegnando ogni elemento a un thread distinto:

Esempio completo di codice:

```c
#include <cuda_runtime.h>
#include <stdio.h>

// Kernel per sommare due array
__global__ void addArrays(int *a, int *b, int *c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

int main() {
    int n = 1024; // Numero di elementi
    size_t size = n * sizeof(int);

    // Allocazione memoria host
    int *h_a = (int *)malloc(size);
    int *h_b = (int *)malloc(size);
    int *h_c = (int *)malloc(size);

    // Inizializzazione degli array host
    for (int i = 0; i < n; i++) {
        h_a[i] = i;
        h_b[i] = i * 2;
    }

    // Allocazione memoria device
    int *d_a, *d_b, *d_c;
    cudaMalloc(&d_a, size);
    cudaMalloc(&d_b, size);
    cudaMalloc(&d_c, size);

    // Copia dei dati da host a device
    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);

    // Configurazione griglia e blocchi
    int threadsPerBlock = 256;
    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;

    // Lancio del kernel
    addArrays<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);

    // Copia dei risultati da device a host
    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);

    // Verifica dei risultati
    for (int i = 0; i < 10; i++) {
        printf("%d + %d = %d
", h_a[i], h_b[i], h_c[i]);
    }

    // Deallocazione memoria
    free(h_a); free(h_b); free(h_c);
    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);

    return 0;
}
```

1. Calcolo dell'indice globale:
```c
int idx = blockIdx.x * blockDim.x + threadIdx.x;
```
2. Operazione parallela:
```c
if (idx < n) c[idx] = a[idx] + b[idx];
```

> **Efficienza:** Migliaia di thread GPU lavorano in parallelo, superando la scalabilità limitata della CPU.

### Mapping degli Indici

Thread e blocchi 2D o 3D sono ideali per elaborare matrici o dati volumetrici:

- Calcolo indice globale 2D:
```c
int idx = threadIdx.x + blockIdx.x * blockDim.x;
int idy = threadIdx.y + blockIdx.y * blockDim.y;
```

---

## Analisi delle Prestazioni

### Profiling

Strumenti come NVIDIA Nsight analizzano l'efficienza dei kernel CUDA, misurando:

1. **Utilizzo delle risorse:** Memoria, unità di calcolo.
2. **Collo di bottiglia:** Trasferimenti PCIe o accessi inefficaci alla memoria.

> **Best Practices:**
> - Minimizzare trasferimenti host-device.
> - Ottimizzare accessi alla memoria globale.
> - Profilare regolarmente per iterare sul codice.

### Verifica del Kernel

Confrontare risultati CPU e GPU per garantire accuratezza:
```c
if (abs(host[i] - gpu[i]) > tol) printf("Errore!");
```

> **Suggerimento:** Lanciare kernel con `<<<1,1>>>` per debugging sequenziale.

---

CUDA offre un framework potente per sfruttare il parallelismo delle GPU, ma richiede ottimizzazioni mirate per garantire prestazioni ottimali. Gestione efficiente della memoria, configurazione accurata di griglie e blocchi, e utilizzo degli strumenti di profiling sono fondamentali per massimizzare l'efficienza.


# Modello di Esecuzione CUDA<div style="text-align: right">[back](./SistemiDigitali.md)</div>

## Indice

- [Modello di Esecuzione CUDAback](#modello-di-esecuzione-cudaback)
  - [Indice](#indice)
  - [Architettura Hardware GPU](#architettura-hardware-gpu)
    - [Introduzione al Modello di Esecuzione CUDA](#introduzione-al-modello-di-esecuzione-cuda)
    - [Streaming Multiprocessor (SM)](#streaming-multiprocessor-sm)
      - [CUDA Core](#cuda-core)
    - [Architettura Fermi (2010)](#architettura-fermi-2010)
    - [Architettura Kepler (2012)](#architettura-kepler-2012)
      - [GK100X SMX](#gk100x-smx)
      - [Evoluzione](#evoluzione)
    - [Tensor Core: Acceleratori di Intelligenza Artificiale (Volta +)](#tensor-core-acceleratori-di-intelligenza-artificiale-volta-)
      - [Evoluzione dei NVIDIA Tensor Core](#evoluzione-dei-nvidia-tensor-core)
  - [Organizzazione e Gestione dei Thread](#organizzazione-e-gestione-dei-thread)
    - [SM, Trehad Blocks e Risorse](#sm-trehad-blocks-e-risorse)
    - [Corrispondenza tra Vista Logica e Vista Hardware](#corrispondenza-tra-vista-logica-e-vista-hardware)
      - [Software](#software)
      - [Hardware](#hardware)
    - [Distribuzione dei Blocchi su Streaming Multiprocessor](#distribuzione-dei-blocchi-su-streaming-multiprocessor)
    - [Scalabilità in CUDA](#scalabilità-in-cuda)
  - [Modello di Esecuzione SIMT e Warp (slide 29)](#modello-di-esecuzione-simt-e-warp-slide-29)
    - [Modello di esecuzione SIMD](#modello-di-esecuzione-simd)
    - [Modello di esecuzione SIMT](#modello-di-esecuzione-simt)
    - [Modello di Esecuzione Gerarchico di CUDA](#modello-di-esecuzione-gerarchico-di-cuda)
  - [Warp: Unità fondamentale di Esecuzione nelle SM](#warp-unità-fondamentale-di-esecuzione-nelle-sm)
    - [Organizzazione dei Thread e Warp](#organizzazione-dei-thread-e-warp)
  - [Compute capability (CC) - Limiti su Blocchi e Thread](#compute-capability-cc---limiti-su-blocchi-e-thread)
  - [Warp: Contesto di Esecuzione](#warp-contesto-di-esecuzione)
  - [Classificazione dei Thread Block e Warp](#classificazione-dei-thread-block-e-warp)
  - [Classificazione degli Stati dei Thread](#classificazione-degli-stati-dei-thread)
  - [Scheduling dei warp](#scheduling-dei-warp)
    - [Warp Scheduler e Dispatch Unit](#warp-scheduler-e-dispatch-unit)
    - [Scheduling dei Warp: TLP e ILP](#scheduling-dei-warp-tlp-e-ilp)
  - [Esecuzione parallela dei warp (FERMI SM)](#esecuzione-parallela-dei-warp-fermi-sm)
  - [Scheduling Dinamico delle Istruzioni (FERMI SM)](#scheduling-dinamico-delle-istruzioni-fermi-sm)
    - [Latency, Throughput e Concurrency](#latency-throughput-e-concurrency)
  - [Latency Hiding nelle GPU](#latency-hiding-nelle-gpu)
    - [Esempio di esecuzione di Blocchi e Warp su un SM](#esempio-di-esecuzione-di-blocchi-e-warp-su-un-sm)
  - [Legge di Little](#legge-di-little)
  - [Warp Divergence](#warp-divergence)
    - [CPU vs GPU: Gestione del Brancing e della Warp Divergence](#cpu-vs-gpu-gestione-del-brancing-e-della-warp-divergence)
    - [Analisi del flusso di esecuzione](#analisi-del-flusso-di-esecuzione)
  - [Serializzazione nella Warp Divergence](#serializzazione-nella-warp-divergence)
  - [Confronto delle condizione di Branch (saltabile slide 64)](#confronto-delle-condizione-di-branch-saltabile-slide-64)

## Architettura Hardware GPU

### Introduzione al Modello di Esecuzione CUDA

In generale un modello di esecuzione fornisce una visione operativa di come le istruzioni vengono eseguite su una specifica architettura di calcolo

**Caratteristiche Principali:**
- Astrazione dell'architettura GPU NVIDIA
- Conservazione dei concetti fondamentali tra le generazioni
- Esposizione delle funzionalità architetturali chiave per la programmazione CUDA
- Basato sul parallelismo massivo e sul modello SIMT (Single Instruction, Multiple Threads)
  
**Importanza:**
- Offre una visione unificata dell'esecuzione su diverse GPU
- Fornisce indicazioni utili per l'ottimizzazione del codice in termini di:
  - Througput delle istruzioni.
  - Accessi alla memoria.
- Facilita la comprensione della relazione tra il modello di programmazione e l'esecuzione effettiva

### Streaming Multiprocessor (SM)

Gli streaming multiprocessor (SM) sono i blocchi di elaborazione principali delle GPU NVIDIA. Ogni SM è composto da:
- Unità di calcolo
- Memoria condivisa
- Risorse (registri, cache, ecc.)

Il parallelismo hardware delle GPU è ottenuto attraverso la replica di questo blocco architetturale

![alt text](image-30.png)

**Caratteristiche Principali:**
1. CUDA Cores: Unità di calcolo che eseguono le istruzioni
2. Shared Memory / L1 Cache: Memoria condivisa tra i thread di uno stesso blocco
3. Register File: Memoria veloce per i registri dei thread
4. Load/Store Units: Unità per l'accesso alla memoria globale
5. Special Function Units: Unità per operazioni speciali (es. funzioni matematiche)
6. Warp Scheduler: Unità per la gestione dei warp
7. Dispatch Unit: Unità per l'assegnazione dei warp ai CUDA Cores
8. Instruction Cache: Memoria per le istruzioni

![alt text](image-31.png)

#### CUDA Core

Un Cuda è l'unità di elaborazione di base all'interno di un SM di una GPU NVIDIA.

![alt text](image-32.png)

**Composizione e Funzionamento (Fermi)**
I CUDA Core erano unità di elaborazione relativamente semplici, in grado di eseguire sia operazioni intere (INT) che in virgola mobile (FP) in un ciclo di clock.
- ALU (Arithmetic Logic Unit): Ogni CUDA Core contiene un'unità logico-aritmetica che esegue operazioni matematiche di base come addizioni, sottrazioni, moltiplicazioni e operazioni logiche.
- FPU (Floating Point Unit): Ogni CUDA Core contiene un'unità in virgola mobile che esegue operazioni in virgola mobile come addizioni, sottrazioni, moltiplicazioni e divisioni.
  
I CUDA Core usano registri condivisi a livello di Streaming Multiprocessor per memorizzare i dati temporanei e i risultati intermedi.

**Evoluzione dell'architettura (da Kepler)**

Dall'architettura Kepler NVIDIA ha introdotto la specializzazione delle unità di calcolo all'interno di un SM:

- **General:**
  - **Unità FP64**: Unità specializzata per operazioni in virgola mobile a doppia precisione
  - **Unità FP32**: Unità specializzata per operazioni in virgola mobile a singola precisione
  - **Unità INT**: Unità specializzata per operazioni intere
- **AI:**
  - **Tensor Core**: Unità specializzata per operazioni di moltiplicazione e accumulo di matrici
- **Graphics:**
  - **Ray Tracing Core (RT Core)**: Unità specializzata per operazioni di ray tracing
  - **Unità di Texture**: Unità specializzata per operazioni di texture mapping
  - **Unità di Rasterizzazione**: Unità specializzata per operazioni di rasterizzazione

Ogni unità di elaborazione esegue un thread in parallelo con altri nel medesimo SM.

### Architettura Fermi (2010)

**Caratteristiche Principali:**
- Prima architettura GPU completa per **applicazioni HPC** ad alte prestazioni.
- Fino a **521 CUDA** cores organizzati in 16 SM.
- Ogni SM contiene:
  - 32 CUDA Cores.
  - 2 unità di scheduling e dispatch.
  - 64KB di Shared Memory/Cache L1.
  - 32.768 registri da 32 bit.
- 768 KB di memoria cache L2 con **coalescenza di memoria**.
- Interfaccia di memoria a **384 bit** con **GDDR5**, supporto fino a **6 GB** di memoria globale.
- GigaThread Engine per la gestione di **migliaia di thread**.
- Interfaccia Host-Device per connessione CPU via PCI Express.

![alt text](image-33.png)

**Esecuzione Concorrente dei Kernel:**
- L'architettura permette l'esecuzione di più kernel in modo concorrente.
- Supporta fino a 16 kernel in esecuzione contemporanea.
- Ottimizza l'uso della GPU per applicazioni con diversi kernel.
- Appare come una architettura MIMD (Multiple Instruction, Multiple Data).
- Le generazioni successive a Fermi supportano un numero ancora maggiore di kernel in esecuzione.

![alt text](image-34.png)

### Architettura Kepler (2012)

**Caratteristiche Principali GPU**
- L'architettura Kepler include 3 importanti novità:
  - Straming Multiprocessors Potenziati (SMX).
  - Dynamic Parallelism: Permette ai kernel di lanciare altri kernel.
  - Hyper-Q: Permette a più CPU di comunicare con la GPU.
  - GPU Boost: Permette di aumentare la frequenza di clock della GPU in base al carico di lavoro.
- 2688 Cuda Corse organizzati in 15 SMX.
- 6 Controller di Memoria a 64 bit.
- 6 GB di Memoria Gloabale DDR5.
- Larghezza di Banda della memoria: 250 GB/s.
- 1536 KB di Cache L2.
- Interfaccia Host-Device PCI Express 3.0.

![alt text](image-35.png)

#### GK100X SMX

**Caratteristiche Principali - Singolo SMX**

- Ogni SMX contiene 192 CUDA Cores, per un totale di 2880 CUDA Cores.
- Unità di precisione:
  - Unità di precisione singla (FP32): 192 CUDA Cores.
  - Unità di precisione doppia (FP64): 64 CUDA Cores.
- 32 Unità di Funzione Speciale (SFU).
- 32 Unità di Load/Store (LD/ST).
- 64 KB di Shared Memory/Cache L1.
- 48 KB di Read-Only Data Cache.
- 65,536 Registri da 32 bit.
- 4 Warp Scheduler.
- 8 Instruction Cache.

![alt text](image-36.png)

#### Evoluzione

![alt text](image-37.png)

![alt text](image-38.png)

### Tensor Core: Acceleratori di Intelligenza Artificiale (Volta +)

I tensor core sono unità di elaborazione specializzata per operazioni tensoriali (array multidimensionali), progettati per accelerari i calcoli di AI e HPC, presenti in GPU NVIDIA RTX da Volta (2017) in poi.

**Caratteristiche Principali:**
- Esegue operazioni matrice-matrice in precisione mista.
- Supporta formati FP16, FP32, FP64, INT8, INT4, BF16 e nuovi formati come TF32.
- Offre un significativo speedup nel calcolo senza compromettere l'accuratezza

- **Fused Multiply-Add (FMA):** Un'operazione che combina una moltiplicazione e una addizione di scalari in un unico passo eseguendo $d=a*b+c$. Un CUDA core esefue 1 FMA per ciclo di clock in FP32.
- **Matrix Multiply and Accumulate (MMA):** Un'operazione che combina una moltiplicazione e una addizione di matrici in un unico passo eseguendo $D+=A*B$. Un Tensor Core esegue 64 FMA per ciclo di clock in FP16.
- Una MMA di dimensione $m*n$ richiede $m*n*k$ operazioni FMA dove $k$ è il numero di colonne di $A$ e il numero di righe di $B$.

![alt text](image-39.png)

> Esecuazione Parallela
> - ogni tensor core esegue 64 FMA in un singolo ciclo
> - Per operazioni su matrici più grandi, queste vengono decomposte in sottomatrici 4x4
> - più operazioni 4x4 vengono eseguite in parallelo su diversi Tensor Core

#### Evoluzione dei NVIDIA Tensor Core

Le generazioni più recenti di CPU hanno ampliato la flessibilità e le prestazioni dei Tensor Core, supportando dimensioni di matrici più grandi con un maggiore numero di formati numerici

![alt text](image-40.png)

- **Acelerazione** significativa dei calcoli
- **Riduzione del consumo** di memoria e energia.
- **Perdita di Precisione**: Si è dimostrato che ha un impatto minimo sulla accuratezza finale dei modelli di deep learning.

## Organizzazione e Gestione dei Thread

### SM, Trehad Blocks e Risorse

- **Parallelismo Hardware**
  - Più SM per GPU permettono l'esecuzione simultanea di migliaia di thread (anche da kernel differenti).
- **Distribuzione dei Thread Block**
  - Quando un kernel viene lanciato, i blocchi vengono distribuiti dal GigaThread Engine ai SM.
  - Le variabili di identificazione e dimensione ```gridDim```, ```blockIdx```, ```blockDim``` e ```threadIdx``` sono rese disponibili ad ogni thread e condivise nello stesso SM.
  - Una volta assegnati a un SM, i thread di un blocco eseguono esclusivamente su quell'SM.
- **Gestione delle Risorse**
  - Più blocchi di Thread possono essere assegnati allo stesso SM contemporaneamente.
  - Lo scheduling dei blocchi dipende dalla disponibilità delle risorse dell'SM e dai limiti architetturali di ciascun SM.
- **Parallelismo Multi-Livello**
  - Parallelismo a livello di istruzioni: Le istruzioni all'interno di un singolo thread sono eseguite in pipeline.
  - Parallelismo a livello di Thread: Esecuzione concorrente di gruppi di threads (warp) su un SM.

### Corrispondenza tra Vista Logica e Vista Hardware

![alt text](image-41.png)

#### Software

- **Thread**:
  - ha uno spazio di memoria privato (registri nell'SM e Memoria locale) per indifie, variabili, e risultati intermedi.

- **Thread Block**:
  - Gruppo di thread eseguiti concorrentemente.
  - Cooperazione tramite barriere di sincronizzazione
  - Usa shared memory per comunicazione inter-thread.
  - Un blocco di thread viene assegnato esclusivamente ad un solo SM.
  - Una volta che un blocco di thread è stato assegnato ad un SM, vi rimane fino al completamento dell'esecuzione.

- **Grid**:
  - Insieme di thread block che eseguono lo stesso kernel.
  - Accesso comune alla global memory.
  - I thread block non possono sincronizzarsi direttamente tra di loro.

#### Hardware

- **SM**:
  - Un SM più contenere più blocchi di thread contemporaneamente.
  - Ogni SM ha un limite massimo di thread block gestibili, determinato dalla sua compute capability.

### Distribuzione dei Blocchi su Streaming Multiprocessor

Supponiamo  di dovere realizzare un algoritmo parallelo che effettuio il calcolo parallelo su una immagine

![alt text](image-42.png)

- Il gigathread Engine smista i blocchi di thread agli SM in base alle risorse disponibili.
- CUDA non garantisce l'ordine e non è possibile scambiare dati tra i blocchi.
- Ogni blocco viene elaborato in modo indipendente.

![alt text](image-43.png)

- Quando un blocco completa l'esecuzione e libera le risorse, un nuovo blocco può essere assegnato allo SM. Questo processo continua fino a quando tutti i blocchi del grid non sono stati elaborati.

### Scalabilità in CUDA

Per scalabilità in CUDA ci si riferisce alla capacità di una applicazione di migliorare le prestazioni proporzionalmente all'aumentodelle risorse hardware disponibili.

Più SM disponibili = più blocchi eseguiti contemporaneamente = Maggiore Parallelismo.

Nessuna modifica al codice richiesta per sfrittare hardware più potente.

![alt text](image-44.png)

## Modello di Esecuzione SIMT e Warp (slide 29)

### Modello di esecuzione SIMD

- È un modello di esecuzione parallela comunemente utilizzato dalle CPU dove una singola istruzione opera simultaneamente su più elementi usando unità di elaborazione vettoriale.
- Utilizza registri vettoriali che possono contenere più elementi
- Il programma segue un flusso di controllo centralizzato (unico thread).
- Limitazioni:
  - Larghezza vettoriale fissa nell'hardware, limitando gli elementi per istruzione.
  - Tutti gli elementi vettoriali in un vettore vengono eseguiti insieme in un gruppo sincrono unificato.
  - Divergenza non è ammessa in SIMD, se sono richieste condizioni, si usano maschere esplicite che indicano su quali elementi il calcolo deve essere eseguito.

![alt text](image-45.png)

### Modello di esecuzione SIMT

- Modello ibrido adottato in CUDA che combina parallelismo a livello di più thread con esecuzione tipo SIMD.
- **Caratteristiche chiave:**
  - A differenza del SIMD, non ha un controllo centralizzato delle istruzioni.
  - Ogni thread possiede un proprio **Program Counter (PC)**, **registri** e **stato** indipendenti.
  - Supporta divergenza del flusso di controllo tra i thread.
- **Implementazione:**
  - In CUDA, i thread sono organizzati in gruppi di 32 chiamati **warp**.
  - I thread in un warp iniziano insieme allo stesso indirizzo del programma (PC), ma possono divergere.
  - Divergenza in un warp causa esecuzione seriale dei percorsi divesi, riducendo l'efficienza.
  - La divergenza è gestita automaticamente dall'hardware, ma con un impatto nevativo sulle prestazioni.
  
![alt text](image-46.png)

> **Perchè 32 thread in un warp CUDA?**
> - **Efficienza Hardware:** Massimizza l'utilizzo delle risorse hardware.
>   - Un warp troppo piccolo sarebbe inefficiente, uno troppo grande complicherebbe lo cheduling e potrebbe sovraccaricare gli SM / la memoria.
> - **Efficienza della Memoria:** Un warp di 32 thread accede a indirizzi di memoria consecutivi, permettendo aggregazioni in poche transazioni e massimizzando l'efficienza delle linee di connessione per evitare accessi parziali.
> - **Flessibilità Software:** Offre una granularità gestibile per il parallelismo e la divergenza dei thread.
> - **Adattabilità:** Questa dimensione si è dimostrata efficace per varie generazioni di GPU NVIDIA, pur rimanendo aperta a future evoluzioni.

![alt text](image-47.png)

### Modello di Esecuzione Gerarchico di CUDA

**Livello di Programmazione**

![alt text](image-48.png)

**Livello di Esecuzione**

![alt text](image-49.png)

## Warp: Unità fondamentale di Esecuzione nelle SM

- **Distribuzione dei Thread Block**:
  - Quando si lancia una griglia di thread block, questi vengono distribuiti tra i diversi SM disponibili.
- **Partizionamento in Warp**:
  - I thread di un thread block venngono suddivisi in warp di 32 thread (con ID consecutivi).
- **Esecuzione SIMT**:
  - I thread in un warp eseguono la stessa istruzione su dati diversi, con possibilità di divergenza.
- **Esecuzione Logica vs Fisica**:
  - Thread eseguiti in parallelo logicamente, ma non sempre fisicamente.
- **Scheduling Dinamico (Warp Scheduler)**
  - L'SM gestisce dinamicamente l'esecuzione di un numero limitato di warp, switchando efficientemente tra essi.
- **Sincronizzazione**
  - Possibile all'interno di un thread block, ma non tra blocchi diversi.
  
![alt text](image-50.png)

### Organizzazione dei Thread e Warp

**Thread Blocks e Warp**

- **Punto di vista logico:** Un blocco di thread è una collezione di thread organizzati in un layout 1D, 2D o 3D.
- **Punto di vista hardware:** Un blocco di thread è una collezione 1D di warp. I thread in un blocco sono organizzati in un layout 1D e ogni insieme di 32 thread consecutivi forma un warp.

![alt text](image-51.png)

![alt text](image-52.png)

![alt text](image-53.png)

---

- Un warp viene assegnato a una sub-partition, solitamente in base al lsuo ID, dove rimane fino al completamento.
- Una sub-partition gestisce un "pool" di warp concorrenti di dimensione fissa.
  
![alt text](image-54.png)

## Compute capability (CC) - Limiti su Blocchi e Thread

- **Compute Capability (CC):** di NVIDIA indica le caratteristiche e la capacità di una GPU in termini di funzionalità supportate e limiti hardware
- È composta da due numeri: il numero principale indica la generazione dell'architettura, il numero secondario indica la versione specifica.

![alt text](image-55.png)

## Warp: Contesto di Esecuzione

- il contesto di esecuzione locale di un warp in SM contiene:
  - Program counter: indica l'indirizzo della prossima istruzione da eseguire.
  - Call stack: struttura dati che memorizza le informazioni sulle chiamate di funzione.
  - Registri: memoria veloce per i dati temporanei e i risultati intermedi.
  - Memoria condivisa: memoria condivisa tra i thread di un blocco.
  - Thread Mask: indica quali thread di un warp sono attivi.
  - Stato di esecuzione: informazioni sullo stato di esecuzione del warp.
  - Warp ID: identificatore che consente di distinguere i warp all'interno di uno SM.
- L'SM mantiene on-chip il contesto di ogni warp per tutta la sua durata, il cambio di contesto è senza costo.

## Classificazione dei Thread Block e Warp

**Thread Block Attivo**
- Un thread block viene considerato attivo quando gli vengoono allocate risorse di calcolo di un SM come registri e memoria condivisa
- I warp contenuti in un thread block attivo sono chiamati warp attivi
- Il numero di blocchi attivi in ciascun istante è limitato dalle risorse dell'SM

**Tipi di Warp Attivi**
- Warp Selezionato
  - Un warp in esecuzione attiva su una unità di elaborazione (FP32, INT32, Tensor Core, ecc.)
- Warp in Stallo
  - Un warp in attesa di dati o risorse, impossibilitato a proseguire l'esecuzione
  - Può essere dovuto a:
    - Latenza di memoria
    - Dipendenze da istruzioni
    - Sincronizzazione
- Warp Eleggibile/Candidato
  - Un warp pronto per l'esecuzione, con risorse necessarie disponibili.
  - Condizioni:
    - Dispobilità di risorse
    - Prontezza dei dati
    - Nessuna dipendenza da istruzioni precedenti bloccante
  
## Classificazione degli Stati dei Thread

**Thread all'interno di un Warp**
- Un warp contiene sempre 32 thread, ma non tutti potrebbero essere logicamente attivi.
- Lo stato di ogni thread è tracciato attraverso una thread mask o maschera di attività

**Stati dei Thread**
- Thread attivo
  - Esegue l'istruzione corrente del warp
  - Contribuisce attivamente all'esecuzione SIMT
- Thread inattivo
  - Divergenza: Ha seguito un percorso diverso nel warp per istruzioni di controllo flusso, come salti condizionali.
  - Terminazione: Ha completato la sua esecuzione prima di altri thread nel warp.
  - Padding: I thread di padding sono utilizzati in situazioni in cui il numero totale dei thread del blocco non è un multiplo di 32, per garantire che il warp sia completamente riempito.

 ## Scheduling dei warp

 **Warp Scheduler**
 - Unità hardware presente in più copie all'interno di ogni SM, responsabile della selezione e assegnazione dei warp alle unità di calcolo CUDA.
 - Obiettivo: Massimizzare l'utilizzo delle risorse di calcolo dell'SM, selezionando in modo efficiente i warp pronti e minimizzando i tempi di inattività.
 - Latency hiding: Contribuiscono a nascondere la latenza eseguendo warp alternativi quando altri sono in stallo, garantendo un utilizzo efficace delle risorse.
  
**Funzionamento Generale**
- Processo di chedulazione:
  - I warp scheduler all'interno di un SM selezionano i warp eleggibili ad ogni ciclo di clock e li inviano alle dispatch unit, responsabili della assegnazione effettiva alle unità di esecuzione.
- Gestione degli stalli:
  - Se un warp è in stallo, il warp scheduler seleziona un altro waep eleggibile per l'esecuzione, garantendo l'esecuzione continua e l'uso ottimale delle risorse di calcolo.
- Cambio di contesto:
  - Il cambio di contesto tra warp è estremamente rapido grazie alla partizione delle risorse di calcolo e alla struttura hardware della GPU.  

### Warp Scheduler e Dispatch Unit

**WARP Scheduler**
- Cervello strategico che decide quali warp mandare in esecuzione
- Monitora continuamente lo stato dei warp per identificare quelli eleggibili
- Gestisce la priorità e l'ordine di esecizione dei warp, cercando di minimizzare le latenze

**Dispatch Unit**
- Ente esecutivo che si occupa di come eseguire i warp selezionati
- Si occupa:
  - Decodificare le istruzioni del warp
  - Distribuire i thread del warp alle unità di calcolo appropriate
  - Recuperare i dati dai registri e dalla memoria necessaria per l'esecuzione
  - Assegnare fisicamente le risorse hardware ai thread

![alt text](image-56.png)

### Scheduling dei Warp: TLP e ILP

- **Thread Level Parallelism (TLP)**
  - Esecuzione simultanea di più warp per sfruttare il parallelismo tra thread.
  - Quando un warp è in attesa, un altro warp viene selezionato ed eseguito aumentando l'occupazione delle unità di calcolo.
- **Instruction Level Parallelism (ILP)**
  - Esecuzione simultanea di più istruzioni indipendenti all'interno di un warp.
  - Ogni warp esegue istruzioni in modo indipendente, sfruttando il parallelismo tra istruzioni.
  - L'ILP è limitato dalla presenza di dipendenze tra le istruzioni, che possono causare stalli.

## Esecuzione parallela dei warp (FERMI SM)

**Componenti chiave per il parallelismo**:
- Due scheduler di warp pronti da eseguire dai thread block assegnati all'SM
- Due Unità di Dispatch delle istruzioni: inviano le istruzioni dei warop selezionati alle unità di esecuzione.

**Flusso di esecuzione**:
- I blocchi vengono assegnati all'SM e divisi in warp
- Due scheduler selezionano warp pronti per l'esecuzione
- Ogni dispatch unit invia un istruzione per warp a 16 CUDA core, 16 unità di caricamento, 4 unità di funzioni speciali
- Questo processo si ripete ciclicamente consentendo l'esecuzione parallela di più warp da più blocchi.

![alt text](image-57.png)

## Scheduling Dinamico delle Istruzioni (FERMI SM)

- Ad ogni ciclo di clock, un warp scheduler emette un'istruzione pronta per l'esecuzione
- L'istruzione può provenire dallo stesso warp (ILP) se indipendente, o più spesso da un warp diverso (TLP)
- Se le risorse sono occupate, lo scheduler passa a un altro warp pronto (latency hiding)

![alt text](image-58.png)

### Latency, Throughput e Concurrency

- **Mean Latency (ML)**
  - La latenza media è la media delle latenze degli elementi individuali. La latenza di un singolo elemento è la differenza tra il suo tempo di inizio e il suo tempo di fine
- **Throughput**
  - Rappresenta la velocità di elaborazione. È definito come il numero degli elementi completati entro un dato intervallo di tempo, diviso per la durata dell'intervallo stesso.
- **Concurrency**
  - Misura quanti elementi vengono processati contemporaneamente in un determinato momento. Si può definire sia istantaneamente che come media su un intervallo di tempo.

![alt text](image-59.png)

## Latency Hiding nelle GPU

- È una tecnica che permette di mascherare i tempi di attesa dovuti ad operazioni ad alta latenza attraverso l'esecuzione concorrente di più warp all'interno di un SM.
- Si ottiene intercambiando la computazione tra warp, per massimizzare l'utilizzo delle unità di calcolo di ogni SM.

**Funzionamento**
- Ogni SM può gestire decine di warp concorrentemente da più blocchi.
- Quando un warp è in stallo l'SM passa immediatamente all'esecuzione di altri warp pronti.
- I warp scheduler dell'SM selezionano costantemente i warp pronti all'esecuzione

**Vantaggi del latency hiding**
- Miglior utilizzo delle risorse: Le unità di elaborazione della GPU sono mantenute costantemente occupate, massimizzando il throughput.
- Maggiore Throughput: Completamente di un maggior numero di operazioni nella stessa unità di tempo.
- Minore latenza Effettiva: Minimizza l'impatto delle operazioni ad alta latenza.

**Meccanismo dei Warp Scheduler**
- L'immagine mostra due warp scheduler che gestiscono l'esecuzione di diversi warp nel tempo.
- Warp scheduler 0 e 1 alternano l'esecuzione di warp diversi per mantenere le unità di elaborazione occupate.
- Quando un warp è in attesa, altri warp vengono eseguiti per nascondere la latenza.
- I periodi di inattività sono minimizzati.
- Questo approccio permette di mascherare i tempi di latenza e aumentare l'efficienza complessiva.
- Risorse pienamente utilizzate quando ogni scheduler ha un warp eleggibile ad ogni ciclo di clock.

![alt text](image-60.png)

### Esempio di esecuzione di Blocchi e Warp su un SM

**Contesto**
- Esecuzione di un kernel con 1000 blocchi ciascuno composto da 128 thread.

**Fasi principali**
- Allocazione ai SM
  - Ogni SM può gestire simultaneamente un numero limitato di blocchi attivi
  - All'interno di ogni blocco attivo, i thread sono suddivisi in warp
- Warp Attivi per SM
  - Se un SM ad esempio gestisce 12 blocchi, vuol dire che saranno presenti 28 warp attibi in esecuzione smistati poi fra i var SM Sub-partition (SMSP)
- Funzionamento del Warp Scheduler
  - I warp scheduler selezionano un warp eleggibile ad ogni ciclo di clock per l'esecuzione
  - In presenza di un warp in stallo, gli scheduler assegnano un warp alternativo pronto all'esecuzione, garantendo la conntinuità operativa.

## Legge di Little

La legge di Little (o teoria delle code) ci aiuta a capire **quanti warp devono essere in esecuzione concorrente** per ottimizzare il latency hiding e mantenere le unità di elaborazione della GPU occupate.

> Warp Richiesti = Latenza x Throughput

- **Latenza:** Tempo medio necessario per completare un'operazione
- **Throughput:** Numero di operazioni completate in un determinato intervallo di tempo
- **Warp Richiesti:** Numero di warp necessari per mantenere le unità di elaborazione della GPU occupate

Indica che per nascondere la latenza è necessario avere un numero sufficiente di warp in esecuzione o pronti per l'esecuzione, in modo che mentre uno è in attesa, altri possano essere eseguiti.

## Warp Divergence

- In in warp, idealmente tutti i thread eseguono la stessa istruzione contemporaneamente per massimizzare il parallelismo SIMT,.
- Tuttavia se una istruzione condizionale porta thread diversi a percorrere rami diversi del codice, si verifica la Warp Divergence.
- In questo caso il warp esegue serialmente ogni ramo, utilizzando una maschera di attività per abilitare/disabilitare i thread.
- La divergenza termina quando i thread riconvergono alla fine del costrutto condizionale.
- La warp divergence può significativamente degradare le prestazioni perchè i thread non vengono eseguiti in parallelo durante la divergenza
- Notare che il fenomeno della divergenza occorre solo all'interno di un warp, non tra warp diversi.

### CPU vs GPU: Gestione del Brancing e della Warp Divergence

![alt text](image-61.png)

### Analisi del flusso di esecuzione

![alt text](image-62.png)

## Serializzazione nella Warp Divergence

- **Divergenza**
  - Quoando i thread di un warp seguono percorsi diversi a causa di un'istruzione condizionale, il warp esegue il ramo in serie, disabilitando i thread non attivi.
- **Località**
  - La divergenza si verifica solo all'interno di un singolo warp.
  - I warp diversi operano indipendentemente
  - I passi condizionali in differenti warp non causano divergenza
- **Impatto**
  - La divergenza può ridurre il parallelismo fino a 32 volte

![alt text](image-63.png)

```c
__global__ void WorstDivergence(int* x) {
   int i = threadIdx.x + blockDim.x * blockIdx.x;
   switch (i % 32) {
       case 0 :
           x[i] = a(x[i]);
           break;
       case 1 :
           x[i] = b(x[i]);
           break;
       . . .
       case 31:
           x[i] = v(x[i]);
           break;
   }
}
```

## Confronto delle condizione di Branch (saltabile slide 64)

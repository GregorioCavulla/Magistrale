<!DOCTYPE html>
<html>
<head>
<title>Q&A.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="qa-sistemi-digitali">Q&amp;A Sistemi Digitali</h1>
<p><a href="./SistemiDigitali.md">Return</a></p>
<hr>
<h2 id="indice">Indice</h2>
<ul>
<li><a href="#qa-sistemi-digitali">Q&amp;A Sistemi Digitali</a>
<ul>
<li><a href="#indice">Indice</a></li>
</ul>
</li>
<li><a href="#modulo-1">MODULO 1</a>
<ul>
<li><a href="#1-che-cos%C3%A8-unarchitettura-eterogenea-perch%C3%A8-labbiamo-esplorata">1. Che cos'è un'architettura eterogenea? Perchè l'abbiamo esplorata?</a></li>
<li><a href="#simd">SIMD</a>
<ul>
<li><a href="#2-descrivi-in-generale-le-caratteristiche-del-paradigma-simd">2. Descrivi in generale le caratteristiche del paradigma SIMD</a></li>
<li><a href="#3-quali-sono-delle-tipiche-istruzioni-supportate-da-un-estensione-simd">3. Quali sono delle tipiche istruzioni supportate da un estensione SIMD?</a></li>
<li><a href="#4-che-cos%C3%A8-una-operazione-con-saturazione-e-come-mai-%C3%A8-utile">4. Che cos'è una operazione con saturazione e come mai è utile?</a></li>
<li><a href="#5-parlami-del-branching-in-simd">5. Parlami del branching in SIMD</a></li>
</ul>
</li>
<li><a href="#dealing-with-real-numbers">Dealing with real numbers</a>
<ul>
<li><a href="#6-descrivi-le-tecniche-principali-di-codifica-dei-reali">6. Descrivi le tecniche principali di codifica dei reali?</a></li>
<li><a href="#7-che-tipi-di-errori-sono-possibili-con-floating-point">7. Che tipi di errori sono possibili con floating-point?</a></li>
<li><a href="#8-parlami-degli-altri-formati-per-i-reali-oltre-a-quelli-definiti-da-ieee-come-mai-si-%C3%A8-sentito-il-bisogno-di-crearne-di-altri">8. Parlami degli altri formati per i reali oltre a quelli definiti da IEEE? Come mai si è sentito il bisogno di crearne di altri?</a></li>
<li><a href="#9-che-altre-tecniche-conosci-per-ridurre-il-memory-footprint-di-un-programma">9. Che altre tecniche conosci per ridurre il memory footprint di un programma?</a></li>
<li><a href="#10-qual%C3%A8-il-caso-duso-di-fixed-point">10. Qual'è il caso d'uso di fixed-point?</a></li>
<li><a href="#11-quando-%C3%A8-necessario-utilizzare-floating-point">11. Quando è necessario utilizzare floating point?</a></li>
</ul>
</li>
<li><a href="#domande-reali">Domande Reali</a>
<ul>
<li><a href="#weight-sharing">Weight Sharing</a></li>
<li><a href="#tecniche-per-ridurre-la-memoria-usata-da-un-software">Tecniche per ridurre la memoria usata da un software</a></li>
<li><a href="#fpga">FPGA</a></li>
<li><a href="#metodi-efficienti-per-rappresentare-reali-su-fpga">Metodi efficienti per rappresentare reali su FPGA</a></li>
<li><a href="#fixed-point">Fixed-point</a></li>
<li><a href="#come-vengono-gestiti-gli-arrotondamenti-nei-float">Come vengono gestiti gli arrotondamenti nei float</a></li>
<li><a href="#cordic-gain">Cordic gain</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#modulo-2">MODULO 2</a>
<ul>
<li><a href="#modello-di-programmazione-cuda">Modello di programmazione CUDA</a>
<ul>
<li><a href="#11-che-cos%C3%A8-il-modello-di-programmazione-cuda">11. Che cos'è il modello di programmazione CUDA?</a></li>
<li><a href="#12-che-cos%C3%A8-un-thread-cuda">12. Che cos'è un thread CUDA</a></li>
<li><a href="#13-qual%C3%A8-il-tipico-workflow-in-cuda">13. Qual'è il tipico workflow in CUDA?</a></li>
<li><a href="#14-come-vengono-organizzati-i-thread-in-cuda">14. Come vengono organizzati i thread in CUDA?</a></li>
<li><a href="#15-come-mai-c%C3%A8-bisogno-di-una-gerarchia-di-thread">15. Come mai c'è bisogno di una gerarchia di thread?</a></li>
<li><a href="#16-che-cos%C3%A8-un-kernel-cuda">16. Che cos'è un kernel CUDA?</a></li>
<li><a href="#17-ci-sono-dei-limiti-per-quanto-riguarda-il-dimensionamento-di-blocchi-e-griglie">17. Ci sono dei limiti per quanto riguarda il dimensionamento di blocchi e griglie?</a></li>
<li><a href="#18-che-influenza-ha-il-dimensionamento-di-blocchi-e-griglie-sulle-performance">18. Che influenza ha il dimensionamento di blocchi e griglie sulle performance?</a></li>
<li><a href="#19-che-influenza-ha-il-mapping-dei-dati-ai-thread-sulle-performance">19. Che influenza ha il mapping dei dati ai thread sulle performance?</a></li>
<li><a href="#20-esistono-diversi-metodi-di-mapping-dei-dati">20. Esistono diversi metodi di mapping dei dati?</a></li>
</ul>
</li>
<li><a href="#modello-di-esecuzione-cuda">Modello di esecuzione CUDA</a>
<ul>
<li><a href="#21-che-cos%C3%A8-il-modello-di-esecuzione">21. Che cos'è il modello di esecuzione?**</a></li>
<li><a href="#22-che-cos%C3%A8-un-sm-e-da-che-cosa-%C3%A8-composto">22. Che cos'è un SM e da che cosa è composto?</a></li>
<li><a href="#23-come-vengono-distribuiti-i-blocchi-tra-i-vari-sm">23. Come vengono distribuiti i blocchi tra i vari SM?</a></li>
<li><a href="#24-parlami-di-simt-e-delle-sue-differenze-con-il-modello-simd">24. Parlami di SIMT e delle sue differenze con il modello SIMD</a></li>
<li><a href="#25-parlami-di-pi%C3%B9-dei-warp-ed-in-particolare-del-warp-scheduling">25. Parlami di più dei warp ed in particolare del warp scheduling</a></li>
<li><a href="#26-parlami-di-come-si-pu%C3%B2-ottenere-il-latency-hiding-massimo">26. Parlami di come si può ottenere il latency hiding massimo</a></li>
<li><a href="#27-che-cos%C3%A8-indipendent-thread-scheduling">27. Che cos'è Indipendent Thread Scheduling?</a></li>
<li><a href="#28-perch%C3%A8-sono-necessarie-le-operazioni-atomiche-in-cuda">28. Perchè sono necessarie le operazioni atomiche in CUDA?</a></li>
<li><a href="#29-che-cos%C3%A8-il-resource-partitioning-in-cuda">29. Che cos'è il resource partitioning in CUDA?</a></li>
<li><a href="#30-che-cos%C3%A8-loccupancy-in-cuda">30. Che cos'è l'occupancy in CUDA?</a></li>
<li><a href="#31-parlami-di-cuda-dynamic-parallelism">31. Parlami di CUDA Dynamic Parallelism</a></li>
</ul>
</li>
<li><a href="#modello-di-memoria-cuda">Modello di memoria CUDA</a>
<ul>
<li><a href="#32-parlami-di-kernel-compute-bound-e-kernel-memory-bound-come-mai-%C3%A8-importante-distinguere-queste-due-categorie">32. Parlami di kernel compute bound e kernel memory bound, come mai è importante distinguere queste due categorie?**</a></li>
<li><a href="#33-come-possiamo-capire-se-un-kernel-%C3%A8-memory-bound-o-compute-bound-che-cos%C3%A8-il-diagramma-di-roofline">33. Come possiamo capire se un kernel è memory bound o compute bound? Che cos'è il diagramma di roofline?</a></li>
<li><a href="#34-che-tipi-di-memoria-esistono-in-cuda">34. Che tipi di memoria esistono in CUDA?</a></li>
<li><a href="#35-come-vengono-trasferiti-i-dati-dalla-memoria-dellhost-alla-memoria-del-device-a-che-cosa-bisogna-stare-attenti-in-questo-processo">35. Come vengono trasferiti i dati dalla memoria dell'host alla memoria del device? A che cosa bisogna stare attenti in questo processo?</a></li>
<li><a href="#36-che-cos%C3%A8-la-memoria-zero-copy">36. Che cos'è la memoria zero copy?</a></li>
<li><a href="#37-che-cosa-sono-unified-virtual-addressing-uva-e-unified-memory-um">37. Che cosa sono Unified Virtual Addressing (UVA) e Unified Memory (UM)?</a></li>
<li><a href="#38-che-cosa-si-intende-con-pattern-di-accesso-alla-memoria-come-mai-%C3%A8-importante-avere-un-pattern-ottimo-per-le-performance-di-un-kernel">38. Che cosa si intende con pattern di accesso alla memoria? Come mai è importante avere un pattern ottimo per le performance di un kernel?</a></li>
<li><a href="#39-puoi-farmi-qualche-esempio-di-utilizzo-della-smem">39. Puoi farmi qualche esempio di utilizzo della SMEM</a></li>
<li><a href="#40-come-si-utilizza-la-smem">40. Come si utilizza la SMEM?</a></li>
<li><a href="#41-che-cos%C3%A8-un-bank-conflict">41. Che cos'è un bank conflict?</a></li>
</ul>
</li>
<li><a href="#domande-reali-1">DOMANDE REALI</a>
<ul>
<li><a href="#differenza-tra-simt-e-simd">Differenza tra SIMT e SIMD</a></li>
<li><a href="#warp-scheduler-e-dispatcher">Warp scheduler e dispatcher</a></li>
<li><a href="#gigatrhread-engine-e-occupancy">gigatrhread engine e occupancy</a></li>
<li><a href="#shared-memory-come-accedere-e-come-allocare">Shared Memory, come accedere e come allocare</a></li>
<li><a href="#occupancy-definizione-teorica-vs-effettiva">Occupancy (Definizione, Teorica vs Effettiva)</a></li>
<li><a href="#zero-copy-memory">Zero-Copy Memory</a></li>
<li><a href="#shared-memory-struttura">Shared Memory struttura</a></li>
<li><a href="#shared-memory-vs-cache-l1">Shared Memory vs Cache L1</a></li>
<li><a href="#unified-memory">Unified Memory</a></li>
<li><a href="#its">ITS</a></li>
<li><a href="#metodi-per-trasferire-memoria-da-host-a-device">Metodi per trasferire memoria da host a device</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="modulo-1">MODULO 1</h1>
<h3 id="1-che-cos%C3%A8-unarchitettura-eterogenea-perch%C3%A8-labbiamo-esplorata">1. Che cos'è un'architettura eterogenea? Perchè l'abbiamo esplorata?</h3>
<p>Un computer con un'architettura eterogenea è un tipo di computer che integra diversi tipi di processori o core di elaborazione al suo interno combinandone i vantaggi (e bypassando i svantaggi, le cose in cui non sono bravo le faccio fare ad un altro più bravo di me). Il computer, avendo a disposizione unità di calcolo eterogenee, può far eseguire su ognuna di esse il task per cui sono più portate velocizzando in questo modo i calcoli e ottenendo flessibilità.</p>
<p>Ad esempio: un computer con GPU e CPU, può eseguire sulla CPU i task sequenziali sfruttando la sua minore latenza delle operazioni, mentre può eseguire sulla GPU i task altamente paralleli sfruttandone l'alto throughput. A questo punto è il programmatore che decide dove far eseguire cosa per ottenere le migliori prestazioni.</p>
<h2 id="simd">SIMD</h2>
<h3 id="2-descrivi-in-generale-le-caratteristiche-del-paradigma-simd">2. Descrivi in generale le caratteristiche del paradigma SIMD</h3>
<p>SIMD è un paradigma di elaborazione di istruzioni in cui una singola istruzione SIMD elabora multipli dati eseguendo su di essi la medesima operazione. Questi dati sono memorizzati in <strong>registri speciali detti estesi</strong> della CPU pensabili a come un array di registri normali.</p>
<p>Questo paradigma:</p>
<ul>
<li>Incrementa il parallelismo agendo a livello dei dati</li>
<li>Può essere utilizzato in combinazione con altre strategie di parallelismo (Pipelining, superscalarità, multithreading, ...)</li>
<li>Richiede un numero di modifiche all'hardware limitato rispetto a SISD con impatto modesto in termini di maggiori risorse utilizzate</li>
<li>Integrazione di ALU/unità di calcolo addizionali (una per ogni lane)</li>
<li>Integrazione dei registri estesi</li>
<li>Integrazione delle nuove istruzioni nel decoder</li>
</ul>
<h3 id="3-quali-sono-delle-tipiche-istruzioni-supportate-da-un-estensione-simd">3. Quali sono delle tipiche istruzioni supportate da un estensione SIMD?</h3>
<p>In generale quelle classiche di un processore, con in aggiunta istruzioni per la manipolazione dei dati all'interno dei registri estesi:</p>
<ul>
<li>Load e store dei registri estesi da/verso la memoria principale di blocchi di dati (Condizione necessaria per l'efficacia del paradigma SIMD è che i registri estesi possano essere letti/scritti agevolmente)</li>
<li>somme, sottrazioni, moltiplicazioni, etc tra registri estesi</li>
<li>operazioni logiche e di confronto bitwise tra registri estesi</li>
<li>manipolazione e riarrangiamento dei dati intra e inter registro esteso (blend, packing, unpacking, ...)</li>
<li>consentire operazioni con saturazione e operazioni molto comuni come SAD (Sum of Absolute Differences) o FMA (Fused Multiply Add)</li>
</ul>
<h3 id="4-che-cos%C3%A8-una-operazione-con-saturazione-e-come-mai-%C3%A8-utile">4. Che cos'è una operazione con saturazione e come mai è utile?</h3>
<p>Una operazione con saturazione è una operazione che non va in overflow/underflow ma &quot;satura&quot; al valore massimo/minimo. è utile quando serve solo sapere se un valore è molto grande o molto piccolo.</p>
<h3 id="5-parlami-del-branching-in-simd">5. Parlami del branching in SIMD</h3>
<p>Con il paradigma SIMD, il branching condizionata al valore di un registro esteso non è supportato in quanto non applicabile (se 3 lane su 8 rispettano la condizione di branching salto o non salto? non ha senso...).</p>
<p>Se si desidera compiere azioni differenti sui dati all'interno di un registro esteso, si utilizzano apposite maschere prodotte da apposite istruzioni di confronto SIMD e operazioni logiche. Queste maschere filtrano le lane che rispettano le &quot;condizioni di branch&quot; e a cui applicare le operazioni desiderate.</p>
<h2 id="dealing-with-real-numbers">Dealing with real numbers</h2>
<h3 id="6-descrivi-le-tecniche-principali-di-codifica-dei-reali">6. Descrivi le tecniche principali di codifica dei reali?</h3>
<p>Esistono principalmente due codifiche:</p>
<ul>
<li>Fixed point:
<ul>
<li>Codifica parte intera e parte decimale di un reale con un numero costante di bit, mantenendo la virgola in una posizione fissa (fixed point)</li>
<li>bit di segno per dati signed</li>
<li>ha un range dinamico molto limitato rispetto a floating point</li>
<li>tuttavia, mantiene una accuratezza (e quindi un errore) costante su tutto il range dei valori rappresentabili</li>
</ul>
</li>
<li>Floating point:
<ul>
<li>Codifica i reali nella loro rappresentazione scientifica. Composto da:
<ul>
<li>Un certo numero di bit per la parte decimale chiamata <strong>mantissa</strong> (la parte intera è sempre 1 dato che siamo in binario e utilizziamo notazione scientifica)</li>
<li>Un certo numero di bit per l'esponente (da qui floating point, la virgola si sposta in base al valore dell'esponente)</li>
<li>un bit di segno</li>
</ul>
</li>
<li>ha un range dinamico molto ampio; rappresenta sia valori molto piccoli che molto grandi</li>
<li>tuttavia, l'accuratezza varia al variare dell'esponente
<ul>
<li>all'aumentare dell'esponente, aumenta in maniera proporzionale anche il valore dell'ULP (Unit in the Last Place) della mantissa. Questo significa che per i numeri grandi in valore assoluto, si ha una accuratezza bassa (è possibile anche che vengano saltati dei valori interi)</li>
<li>al diminuire dell'esponente invece accade il contrario. Per numeri piccoli in valore assoluto, si ha una accurattezza tanto più precisa tanto quanto è basso il valore dell'esponente (questo causa la necessità di valori subnormali).</li>
<li><strong>TRUCCO</strong>: Un modo di pensarla è che stiamo dividendo sempre nello stesso numero di bin (definito dal numero di bit della mantissa) un intervallo di valori che varia in grandezza al variare dell'esponente</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Floating-point è senza discussione il formato più usato a causa del suo largo range dinamico. Tuttavia, fixed-point trova un caso d'uso nel rimpiazzare quelle che sarebbero costose operazioni floating-point, con operazioni tra interi.</p>
<p><strong>NB</strong>: entrambi i formati sono solo una astrazione approssimata dei reali in quanto con un numero finito di bit è ovvio che non posso rappresentare l'infinito non numerabile dei reali.</p>
<h3 id="7-che-tipi-di-errori-sono-possibili-con-floating-point">7. Che tipi di errori sono possibili con floating-point?</h3>
<ul>
<li><strong>Errori di codifica/approssimazione</strong>: tutti quei valori reali con non ricadono in uno dei '<em>bin</em>' possibili definiti dai bit di mantissa ed esponente, vengono per forza approssimati al bin più vicino.
<ul>
<li>Aumentando il numero di bit della mantissa avremo a disposizione un ULP con un valore sempre più 'fine' e quindi una accuratezza maggiore; tuttavia, rimaranno sempre dei valori che non saranno esprimibili come potenze di due e che quindi dovranno per forza venire approssimati (i.e. 1,21).</li>
<li>Per capire &quot;quale bin è il più vicino&quot; è necessario usare durante la codifica un ulteriore bit di rounding; in questo modo, l'errore massimo sarà val(ULP)/2</li>
<li>questi errori di approssimazione si propagano poi nelle operazioni aritmetiche</li>
<li>(quanto detto sopra vale in realtà anche per fixed-point)</li>
</ul>
</li>
<li><strong>Trocamenti durante le Somme</strong>:
<ul>
<li>Il risultato di una somma tra due numeri a n bit, ha n+1 bit. Questo bit aggiuntivo va troncato e causa un'ulteriore approssimazione.</li>
<li><strong>Allineamento degli esponenti durante le somme</strong>: sommare un numero molto grande con un numero molto piccolo causa dei troncamenti nei due addendi in quando è necessario allineare i due esponenti. è anche possibile, nel caso la differenza tra i due valori sia molto grande, che uno dei due addendi sparisca.</li>
</ul>
</li>
<li><strong>Troncamenti durante le Moltiplicazioni</strong>:
<ul>
<li>Il risultato di un prodotto tra due numeri a n bit, ha 2*n bit. Questo bit aggiuntivi vanno troncati, questo è causa di un'ulteriore approssimazione.</li>
</ul>
</li>
</ul>
<p>In generale, a causa di questi errori bisogna stare attenti a fare confronti di uguaglianza tra float, più safe usare un <em>epsilon</em>.
Inoltre proprità associativa e distributiva di somme e prodotti non sono realmente valide; questo può causare errori difficili da debuggare.</p>
<h3 id="8-parlami-degli-altri-formati-per-i-reali-oltre-a-quelli-definiti-da-ieee-come-mai-si-%C3%A8-sentito-il-bisogno-di-crearne-di-altri">8. Parlami degli altri formati per i reali oltre a quelli definiti da IEEE? Come mai si è sentito il bisogno di crearne di altri?</h3>
<p>In molti casi d'uso (i.e. training di modelli di AI), non è necessaria una alta accuratezza ma si ha bisogno di un largo range dinamico. Appurato cio, diventerebbe allora conveniente compattare la codifica dei float in formati più piccoli, che sacrificano molti bit di mantissa e pochi (se non zero) bit di esponente. Questo porterebbe a tre principali vantaggi:</p>
<ul>
<li><strong>Consumo energetico ridotto</strong>: il consumo di energia per operazioni aritmetiche con float scala con (circa) il quadrato della lungehzza della mantissa</li>
<li><strong>Computazioni più veloci</strong>: con un formato che usa meno bit ho che
<ul>
<li>posso trasferire più dati con la stessa bandwidth riducendo lo stallo delle unità di calcolo per operazioni memory bound (Molta sinergia con il calcolo parallelo).
<ul>
<li>detta in altri termini, aumenta la mia Intensità Aritmetica il che mi sposta più a destra nel diagramma di roofline. Questo mi fa ottenere più FLOPS nel caso in cui fossi stato memory bound.</li>
</ul>
</li>
<li>nella stessa cache ci stanno più dati</li>
</ul>
</li>
<li>**Memory footprint ridotto a runtime</li>
</ul>
<p>Un esempio che può sembrare estremo, ma sorprendentemente efficace, di questi formati è: <em><strong>minifloat E5M2</strong></em>. Con 128 bin (8-1 bit togliendo il bit di segno) ha un range dinamico pari a [-(1+3/4)2^15, +(1+3/4)2^15]</p>
<h3 id="9-che-altre-tecniche-conosci-per-ridurre-il-memory-footprint-di-un-programma">9. Che altre tecniche conosci per ridurre il memory footprint di un programma?</h3>
<p>Abbiamo anche parlato di <em>weight sharing (paletizzazione)</em>. Questa tecnica consiste in:</p>
<ul>
<li>quantizzazione dei dati in un unico valore (media dell'intervallo quantizzato, centroide)</li>
<li>salvataggio dei centroidi in una LUT</li>
<li>a runtime, utilizzo gli indici della LUT (pochi bit per indice) piuttosto che i valori reali che utilizzerebbero più bit</li>
</ul>
<h3 id="10-qual%C3%A8-il-caso-duso-di-fixed-point">10. Qual'è il caso d'uso di fixed-point?</h3>
<p>Fixed-point permette di effettuare calcoli con dati reali utilizzando solo operazioni tra interi. L'idea consiste in:</p>
<ol>
<li>trasformare i dati,  inizialmente in fixed-point, in formato intero mediante uno shift pari al numero di cifre decimali (molto facile con fixed-point)</li>
<li>effettuare le operazioni desiderate con i valori interi</li>
<li>recuperare il risultato reale facendo lo shift inverso alla fine</li>
</ol>
<p>Con questa tecnica si hanno due vantaggi:</p>
<ul>
<li>Speedup nel calcolo: operazioni tra interi richiedono un numero di clock minore rispetto ad operazioni tra float</li>
<li>Non c'è bisogno di unità di calcolo FP: quest'ultime potrebbero essere non disponibili (microcontrollori) oppure troppo costose in termini di spazio/potenza consumata(FPGA)</li>
</ul>
<p><strong>NOTA</strong>: La tecnica del passare al mondo degli interi con una moltiplicazione per un fattore appropriato in realtà si può applicare anche nel contesto floating-point. In questo caso però, il range dinamico molto largo porta ad avere dei fattori moltiplicativi enormi se si moltiplica, ad esempio, un float piccolo (in valore assoluto) per uno molto grande. Questo porta ad ottenere dei valori interi che hanno bisogno di un numero di bit enorme per essere rappresentati e fa perdere i vantaggi del passare alla rappresentazione intera.</p>
<h3 id="11-quando-%C3%A8-necessario-utilizzare-floating-point">11. Quando è necessario utilizzare floating point?</h3>
<p>Utilizzare floating point è necessaria quando si ha bisogno del suo elevato range dinamico. In generale, se si riesce ad evitare (ad esempio utilizzando la tecnica possibile con fixed-point) è meglio in quanto è costoso, sia in termini di performance, che di energia consumata.</p>
<p>Se si ha la necessità di usare floating-point, è importante utilizzare il formato più compatto possibile. Se invece floating point non è strettamente necessario, potrebbe essere più intelligente utilizzare fixed-point e passare al mondo degli interi.</p>
<h2 id="domande-reali">Domande Reali</h2>
<h3 id="weight-sharing">Weight Sharing</h3>
<h3 id="tecniche-per-ridurre-la-memoria-usata-da-un-software">Tecniche per ridurre la memoria usata da un software</h3>
<h3 id="fpga">FPGA</h3>
<h3 id="metodi-efficienti-per-rappresentare-reali-su-fpga">Metodi efficienti per rappresentare reali su FPGA</h3>
<h3 id="fixed-point">Fixed-point</h3>
<h3 id="come-vengono-gestiti-gli-arrotondamenti-nei-float">Come vengono gestiti gli arrotondamenti nei float</h3>
<h3 id="cordic-gain">Cordic gain</h3>
<h1 id="modulo-2">MODULO 2</h1>
<h2 id="modello-di-programmazione-cuda">Modello di programmazione CUDA</h2>
<h3 id="11-che-cos%C3%A8-il-modello-di-programmazione-cuda">11. Che cos'è il modello di programmazione CUDA?</h3>
<p>Il Modello di Programmazione definisce la struttura e le regole per sviluppare applicazioni parallele su GPU. In particolare definisce:</p>
<ul>
<li>Gerarchia di Thread: organizza l'esecuzione parallela in thread, blocchi e griglie, ottimizzando la scalabilità su diverse GPU.</li>
<li>Gerarchia di Memoria: Offre tipi di memoria (globale, condivisa, locale, costante, texture) con diverse prestazioni e scopi, per ottimizzare l'accesso ai dati.</li>
<li>API: Fornisce funzioni e librerie per gestire l'esecuzione del kernel, il trasferimento dei dati e altre operazioni essenziali.</li>
</ul>
<h3 id="12-che-cos%C3%A8-un-thread-cuda">12. Che cos'è un thread CUDA</h3>
<p>Un thread CUDA rappresenta un'unità di esecuzione elementare nella GPU. Ogni thread CUDA si occupa di un piccolo pezzo del problema complessivo, eseguendo calcoli su un sottoinsieme di dati in maniera sequenziale. Il parallelismo si ottiene coprendo l'intero spazio dei dati del problema, lanciando contemporaneamnete migliaia di thread (SIMT). Ogni thread esegue lo stesso codice del kernel ma opera su dati diversi, determinati dai suoi identificatori univoci (threadIdx,blockIdx).</p>
<h3 id="13-qual%C3%A8-il-tipico-workflow-in-cuda">13. Qual'è il tipico workflow in CUDA?</h3>
<ol>
<li>Inizializzazione e Allocazione Memoria su CPU e GPU</li>
<li>Trasferimento Dati (Host → Device)</li>
<li>Esecuzione (asincrona) del Kernel (Device)</li>
<li>(eventuali calcoli lato host)</li>
<li>Recupero Risultati (Device → Host)</li>
<li>Post-elaborazione (Host)</li>
<li>Liberazione Risorse</li>
</ol>
<h3 id="14-come-vengono-organizzati-i-thread-in-cuda">14. Come vengono organizzati i thread in CUDA?</h3>
<p>Abbiamo due livelli di organizzazione:</p>
<ul>
<li>i thread vengono raggruppati in blocchi</li>
<li>i blocchi vengono raggruppati in griglie
Entrambe le struttura possono poi essere ulteriormente strutturate in maniera 1D, 2D o 3D in base al problema da risolvere.</li>
</ul>
<h3 id="15-come-mai-c%C3%A8-bisogno-di-una-gerarchia-di-thread">15. Come mai c'è bisogno di una gerarchia di thread?</h3>
<p>La gerarchia di thread permette di scomporre problemi complessi in unità di lavoro parallele più piccole e gestibili, rispecchiando spesso la struttura intrinseca del problema stesso. Ad esempio si possono distinguere sottoproblemi paralleli diversi in griglie diverse e la griglia può essere strutturata al suo interno in blocchi che logicamente rispecchiano la risoluzione del sottoproblema.</p>
<p>Il programmatore poi, può controllare la dimensione dei blocchi (e della griglia) per adattare l'esecuzione alle caratteristiche specifiche dell'hardware e del problema, ottimizzando l'utilizzo delle risorse della GPU a disposizione. La gerarchia risulta quindi scalabile e permette di adattare l'esecuzione a GPU con diverse capacità e numero di core. Il codice CUDA, quindi, risulta più portabile e può essere eseguito su diverse architetture GPU.</p>
<p>Inoltre, la distinzione tra blocchi e griglie permette operazioni, come sincronizzazione e allocazione di memoria condivisa, che sarebbero troppo costose a livello globale.</p>
<h3 id="16-che-cos%C3%A8-un-kernel-cuda">16. Che cos'è un kernel CUDA?</h3>
<p>Un kernel CUDA è una funzione che viene eseguita in parallelo sulla GPU da migliaia/milioni di thread. Al suo interno vi è definito che cosa il singolo thread dovrà fare, ed il mapping ai dati che dovrà elaborare.</p>
<h3 id="17-ci-sono-dei-limiti-per-quanto-riguarda-il-dimensionamento-di-blocchi-e-griglie">17. Ci sono dei limiti per quanto riguarda il dimensionamento di blocchi e griglie?</h3>
<p>Si, il numero massimo totale di thread per blocco è 1024 per la maggior parte delle GPU. Inoltre, le dimensioni di griglie e blocchi (anche 3D) sono limitate. I valori variano in base a alla compute capability della GPU.</p>
<p>La Compute Capability (CC) di NVIDIA è un numero che identifica le caratteristiche e le capacità di una GPU NVIDIA in termini di funzionalità supportate e limiti hardware.</p>
<h3 id="18-che-influenza-ha-il-dimensionamento-di-blocchi-e-griglie-sulle-performance">18. Che influenza ha il dimensionamento di blocchi e griglie sulle performance?</h3>
<p>Il dimensionamento di blocchi è griglie ha conseguenze dirette sull'utilizzo delle risorse della GPU e sull'occupancy raggiungibile.</p>
<p>Ad esempio:</p>
<ul>
<li>una configurazione con una manciata (&lt; num SM) di blocchi con tanti thread, non attiva tutti gli SM della GPU diminuendo il parallelismo.</li>
<li>al contrario, una configurazione con tanti blocchi composti da una manciata di thread causa un overhead eccessivo per lo scheduler dei blocchi e <strong>potrebbe diminuire l'occupancy siccome si è limitati dal numero di blocchi assegnabile ad un SM</strong>.</li>
</ul>
<p>Altri fattori possono essere:</p>
<ul>
<li>blocchi più grandi che accedono a dati localmente vicini possono anche sfruttare meglio la cache L1 rispetto a blocchi più piccoli.</li>
<li>blocchi con dimensione non multipla di 32 causano l'esecuzione di warp con molti thread disabilitati con spreco delle unità di calcolo</li>
</ul>
<h3 id="19-che-influenza-ha-il-mapping-dei-dati-ai-thread-sulle-performance">19. Che influenza ha il mapping dei dati ai thread sulle performance?</h3>
<p>Innanzitutto, è ovvio che il mapping dei dati deve essere corretto, ovvero bisogna garantire che il mapping permetta di ottenere   un risultato del calcolo parallelo uguale a quello del calcolo sequenziale. Per fare questo il mapping deve: garantire una copertura completa dei dati senza ripetizioni.</p>
<p>Più interessante è il fatto che il mapping dei dati influenzi la scalabilità ed il parallelismo che si riesce ad ottenere da un kernel. Con un mapping inappropriato il kernel potrebbe non scalare al crescere della dimensione dei dati (pensa all'esempio del kernel in cui un thread  somma un'intera colonna tra due matrici) oppure potrebbe diventare proprio impossibile risolvere il problema (pensa ad un mapping con solo threadId -&gt; si è limitati dalla dimensione del blocco).</p>
<p>Altri aspetti influenzati dal mapping dei dati sono: l'accesso alla memoria che idealmente deve essere allineato e coalescente, e il bilanciamento del carico tra i thread che idealmente deve essere uniforme.</p>
<p>Riassumendo il mapping deve garantire:</p>
<ul>
<li>Copertura completa dei dati</li>
<li>Scalabilità per diverse dimensioni dei dati</li>
<li>Coerenza dei risultati con l'elaborazione sequenziale</li>
<li>Accesso efficiente alla memoria</li>
</ul>
<h3 id="20-esistono-diversi-metodi-di-mapping-dei-dati">20. Esistono diversi metodi di mapping dei dati?</h3>
<p>Si, noi ne abbiamo visti due:</p>
<ul>
<li>metodo lineare: più comodo quando si ha a che fare con configurazioni 1D</li>
<li>metodo per coordinate: più comodo quando si ha a che fare con configurazioni 2D/3D
Metodi diversi possono produrre indici globali differenti per lo stesso thread, impattando in questo modo le prestazione del kernel per aspetti come la coalescenza degli accessi in memoria.</li>
</ul>
<h2 id="modello-di-esecuzione-cuda">Modello di esecuzione CUDA</h2>
<h3 id="21-che-cos%C3%A8-il-modello-di-esecuzione">21. Che cos'è il modello di esecuzione?**</h3>
<p>Il modello di esecuzione è un modello che fornisce una visione di come i kernel lanciati lato host vengano effettivamente eseguiti sulla GPU. Studiare il modello di esecuzione è utile in quanto:</p>
<ol>
<li>Fornisce indicazioni utili per l'ottimizzazione del codice</li>
<li>Facilita la comprensione della relazione tra il modello di programmazione e l'esecuzione effettiva.</li>
</ol>
<h3 id="22-che-cos%C3%A8-un-sm-e-da-che-cosa-%C3%A8-composto">22. Che cos'è un SM e da che cosa è composto?</h3>
<p>Gli SM sono dei processori, ovvero cio che esegue le istruzioni specificate dai thread.
Ogni SM, al suo interno, contiene:</p>
<ul>
<li>
<p>diverse <strong>unità di calcolo</strong> (INT unit, FP32 unit, FP64 unit, SFU, Tensor Core, ecc...), ognuna delle quali è in grado di eseguire un thread in parallelo con altri nel medesimo SM (Un SM ospita più Blocchi e quindi multipli warp/thread).</p>
</li>
<li>
<p>almeno una coppia (ma di solito di più) di <strong>warp-scheduler</strong> e <strong>dipatch unit</strong>; queste due unità si occupano rispettivamente di: selezionare quali sono i warp pronti all'esecuzione (all'interno di un blocco assegnato al SM) e di assegnare effettivamente ai warp selezionati le unità di calcolo appropriate.</p>
</li>
<li>
<p>un'insieme di registri, che vengono spartiti ai thread in esecuzione all'interno dell'SM per la memorizzazione ed il calcolo di dati temporanei.</p>
</li>
<li>
<p>Shared memory/L1 cache: una memoria super veloce condivisa tra i thread di un blocco (ecco perchè la shared memory è shared solo all'interno del blocco -&gt; è fisicamente presente solo all'interno del SM in cui il blocco è stato assegnato).
La stessa memoria è divisa tra shared memory (cache programmabile) e cache, ed è il programmatore a decidere/consigliare quanta memoria assegnare all'una rispetto all'altra in base al problema da risolvere.</p>
</li>
</ul>
<p>In realtà, questi sono i componenti principali di SM vecchi. Gli SM delle GPU moderne suddividono poi gli SM in vari SMSP, e sono loro ad essere fatti così. In questo modo si aumenta ulteriormente il parallelismo disponibile a livello di hardware.</p>
<p>Infine, a livello di architettura di GPU globale, sono notevoli anche:</p>
<ul>
<li>la cache L2: che è condivisa tra tutti gli SM e (quindi tutti i blocchi)</li>
<li>il giga thread engine:  Scheduler globale per la distribuzione dei blocchi.</li>
</ul>
<h3 id="23-come-vengono-distribuiti-i-blocchi-tra-i-vari-sm">23. Come vengono distribuiti i blocchi tra i vari SM?</h3>
<ul>
<li>Quando un kernel viene lanciato, i blocchi di vengono automaticamente e dinamicamente distribuiti dal GigaThread Engine agli SM.</li>
<li>Le variabili di identificazione e dimensione: gridDim, blockIdx, blockDim, e threadIdx sono rese disponibili ad ogni thread</li>
<li>Una volta assegnati a un SM, i thread di un blocco eseguono esclusivamente su quell'SM.</li>
<li>Più blocchi possono essere assegnati allo stesso SM contemporaneamente.</li>
<li><strong>Lo scheduling dei blocchi dipende dalla disponibilità delle risorse dell'SM (registri, memoria condivisa) e dai limiti architetturali di ciascun SM (max blocks per SM, max threads per SM, max warp per SM)</strong> (Gigathread engine fa load balancing)</li>
<li>Parallelismo multi-livello nell'esecuzione:
<ul>
<li>Parallelismo a Livello di Istruzione: Le istruzioni all'interno di un singolo thread sono eseguite in pipeline.</li>
<li>Parallelismo a Livello di Thread: Esecuzione concorrente di gruppi di thread (warps) sugli SM (SIMT).</li>
<li>(considerando l'intera GPU, abbiamo anche parallelismo a livello di griglia: SM diversi possono processare blocchi appartenenti a griglie diverse)</li>
</ul>
</li>
</ul>
<h3 id="24-parlami-di-simt-e-delle-sue-differenze-con-il-modello-simd">24. Parlami di SIMT e delle sue differenze con il modello SIMD</h3>
<p>SIMT è un modello di esecuzione dei thread adottato in CUDA in cui:</p>
<ul>
<li>Per prima cosa, i thread di un blocco vengono divisi in <strong>warp</strong>, ovvero gruppi di thread di dimensione fissa (32)</li>
<li>Successivamente (similmente a quanto accade in SIMD) tutti i thread di uno stesso warp eseguono la stessa istruzione</li>
<li>La differenza principale con SIMD sta nel fatto che questo modello ammette divergenza!
<ul>
<li>Quando thread appartenenti allo stesso warp divergono si ha semplicemente esecuzione seriale dei due percorsi.</li>
<li>Quando si intraprende un percorso i thread che non appartengono a quest'ultimo vengono disabilitati</li>
<li>Questa esecuzione seriale fa perdere parallelismo e quindi seppure la divergenza sia possibile, è lo stesso da evitare (diminuisce la branch efficiency)</li>
</ul>
</li>
</ul>
<h3 id="25-parlami-di-pi%C3%B9-dei-warp-ed-in-particolare-del-warp-scheduling">25. Parlami di più dei warp ed in particolare del warp scheduling</h3>
<p>Innanzitutto abbiamo che:</p>
<ul>
<li>Un warp viene assegnato a una sub-partition (dell'SM del blocco a cui appartiene) dove rimane fino al completamento.</li>
<li>Una sub-partition gestisce un “pool” di warp concorrenti di dimensione fissa (es., Turing 8 warp, Volta 16 warp).
<ul>
<li>altro limite architetturale definito dalla CC</li>
</ul>
</li>
</ul>
<p>Successivamente, si ha che <strong>un warp ha un contesto di esecuzione</strong> (similmente ad un processo in un normale SO), esso contiene:</p>
<ul>
<li>Warp ID (PID)</li>
<li>PC (per thread per &gt;=Volta)</li>
<li>Stack (per thread per &gt;=Volta)</li>
<li>Blocchi di Registri e Shared memory (l'offset viene calcolato grazie al warpid)</li>
<li>Stato di esecuzione (in esecuzione, pronto, in stallo)</li>
<li>Thread-mask
<strong>NB</strong>: Notevole il fatto che questo contesto venga salvato on-chip per tutta la durata d'esecuzione del warp. In questo modo <strong>il cambio di contesto è senza costo</strong> quando si vuole eseguire un altro warp (operazione fondamentale per latency hiding)</li>
</ul>
<p>L'attività di warp scheduling consiste nel selezionare dal pool dei warp attivi, un warp appartenente al sottoinsieme dei warp pronti da mandare in esecuzione su un SMSP. Similmente ad uno scheduler classico di un normale SO, se un warp in esecuzione entra in stallo, viene fatto immediatamente un cambio di contesto (senza costo) e viene messo in esecuzione un altro warp. Questo meccanismo è alla base del <strong>latency hiding</strong> e permette di mantere alta l'occupazione delle risorse del SM nascondendo la latenza delle operazioni costose come gli accessi alla memoria globale.</p>
<p>Le unità che si occupano del warp scheduling sono:</p>
<ul>
<li>I warp scheduler all'interno di un SMSP che selezionano i warp eleggibili ad ogni ciclo di clock e li inviano alle dispatch unit</li>
<li>Le dispatch unit, responsabili dell’assegnazione effettiva alle unità di esecuzione
Più Warp scheduler e dispatch unit si ha disposizione all'interno di un SMSP, più in fretta si riesce a riempire quest'ultimo e più in fretta si riesce a sostituire molti warp che entrano in stallo.</li>
</ul>
<p>La tecnica di sopra rientra nella categoria del Thread Level Parallelism. Un'ulteriore tecnica adottabile (ILP) consiste nel fare emettere al warp scheduler istruzioni indipendenti apparteneti allo stesso warp.</p>
<p>TLP e ILP contribuiscono a mantenere le unità di calcolo attive e occupate riducendo i tempi morti dovuti alle operazioni a latenza elevata come accessi alla memoria globale (latency hiding).</p>
<h3 id="26-parlami-di-come-si-pu%C3%B2-ottenere-il-latency-hiding-massimo">26. Parlami di come si può ottenere il latency hiding massimo</h3>
<p>Siccome il latency hiding si ottiene sostituendo il warp correntemente in stallo con un warp pronto, una condizione necessaria per massimizzare quest'ultimo è avere a disposizione &quot;tanti&quot; warp pronti.</p>
<p>Una formalizzazione di questo concetto più operativa è data dalla <strong>Legge di Little</strong>. Questa legge  ci aiuta a calcolare quanti warp (approssimativamente) devono essere in esecuzione/pronti per ottimizzare il latency hiding e mantenere le unità di elaborazione della GPU occupate.</p>
<pre><code>Warp Richiesti = Latenza × Throughput
</code></pre>
<p>Con:</p>
<ul>
<li>Latenza = Tempo di completamento di un'istruzione (in cicli di clock).</li>
<li>Throughput = Numero di warp (e, quindi 32 operazioni) eseguiti per ciclo di clock.</li>
<li>Warp Richiesti = Numero di warp pronti necessari per nascondere la latenza ed ottenere il throughput desiderato</li>
</ul>
<p>Inoltre, una ulteriore strategia possibile è cercare di scrivere operazioni all'interno del kernel il più possibile indipendenti in modo che possano essere emesse in sequenza dal warp scheduler(vedi ILP). Questo è molto <strong>sinergistico con il loop unrolling</strong>, che di suo riduce il numero di istruzioni di controllo da eseguire, ma inoltre aumenta il numero di operazioni indipendenti (vedi riduzione parallela)</p>
<h3 id="27-che-cos%C3%A8-indipendent-thread-scheduling">27. Che cos'è Indipendent Thread Scheduling?</h3>
<p>Prima di ITS il livello di concorrenza minimo era tra Warp siccome era l'intero warp ad avere un PC ed uno stack. Con ITS ogni thread mantiene il proprio stato di esecuzione, inclusi program counter e stack. Di conseguenza, dopo ITS, il livello di concorrenza minimo diventa quello dei singoli thread, anche appartenenti a warp diversi o a rami diversi dello stesso warp.</p>
<p>Prima di ITS</p>
<ul>
<li>Quando c'è divergenza, i thread che prendono branch diverse perdono concorrenza fino alla riconvergenza.</li>
<li>Possibili deadlock tra thread in un warp, se i thread dipendono l'uno dall'altro in modo circolare.
Con ITS entrambe queste situazioni vengono mitigate:</li>
<li>Posso eseguire concorrentemente(non parallelamente) istruzioni appartenenti a rami diversi all'interno di un warp</li>
<li>Un ramo può attendere un altro ramo
Infine, un ottimizzatore di scheduling raggruppa i thread attivi dello stesso warp in unità SIMT mantenendo l'alto throughput dell'esecuzione SIMT, come nelle GPU NVIDIA precedenti.</li>
</ul>
<h3 id="28-perch%C3%A8-sono-necessarie-le-operazioni-atomiche-in-cuda">28. Perchè sono necessarie le operazioni atomiche in CUDA?</h3>
<p>Quando più thread accedono e modificano la stessa locazione di memoria contemporaneamente si ha una corsa critica che produce risultati imprevedibili. Le operazioni atomiche garantiscono la correttezza del risultato impattando pesantemente però sulle performance siccome i thread vengono sequenzializzati durante l'esecuzione di quest'ultima.</p>
<p>Una soluzione a questo problema è duplicare i dati sulla SMEM limitando la sequenzializzazione a livello di blocco. Questa idea ci è stata anche mostrata dal professor Mattoccia nel contesto di OpenMP ed è quindi applicabile anche al di fuori di CUDA.</p>
<h3 id="29-che-cos%C3%A8-il-resource-partitioning-in-cuda">29. Che cos'è il resource partitioning in CUDA?</h3>
<p>Il Resource Partitioning riguarda la suddivisione e la gestione delle risorse hardware limitate all'interno di una GPU, in particolare all'interno di ogni SM. L'obiettivo è massimizzare l'occupancy ed il parallelismo (permettendo l'assegnamente di più blocchi possibile all'interno di un SM)</p>
<p>Le risorse richieste da un blocco (e quindi da partizionare dentro ad un SM) sono tre:</p>
<ol>
<li>La dimensione del blocco, ovvero il numero di thread che devono essere concorrenti (in realtà anche numero di blocchi e numero di warp).</li>
<li>Memoria Condivisa</li>
<li>Registri</li>
</ol>
<p>In caso di problemi di occupancy, ridimensionare la quantità di smem, oppure il numero di registri necessari ad un thread, potrebbe risolvere il problema permettendo l'assegnamento di più blocchi allo stesso SM. <strong>Cio che si desidera è raggiungere il numero di thread massimo gestibile dall'SM</strong> (corrisponde ad una occupancy del 100%).</p>
<h3 id="30-che-cos%C3%A8-loccupancy-in-cuda">30. Che cos'è l'occupancy in CUDA?</h3>
<p>L'occupancy è definita come il rapporto tra i warp attivi e il numero massimo di warp supportati per SM</p>
<pre><code>Occupancy [%] = Active Warps / Maximum Warps
</code></pre>
<p>Una occupancy alta permette di fare latency hiding siccome ci saranno molti warp disponibili a sostituire quelli entrati in stallo;
è chiaro che con un resource partitioning infelice il numero di thread occupati (warp) sarà molto minore rispetto al massimo supportato e questo si rifletterà sull'occupancy.</p>
<p>Alcuni fattori che possono abbassare l'occupancy sono:</p>
<ul>
<li>Dimensioni del blocco di thread: Se i blocchi sono troppo piccoli, si potrebbe essere limitati dal numero massimo di blocchi assegnabili ad un SM e non raggiungere il numero massimo di warps attivi.</li>
<li>Uso dei registri: Se un kernel utilizza troppi registri per thread, il numero totale di blocchi assegnabili ad un SM sarà limitato. Si potrebbe diminuire la dimensione del blocco ma questo causa il problema di sopra.</li>
<li>Memoria condivisa: Se un kernel utilizza molta memoria condivisa, potrebbe ridurre il numero di blocchi assegnabili un SM.</li>
</ul>
<h3 id="31-parlami-di-cuda-dynamic-parallelism">31. Parlami di CUDA Dynamic Parallelism</h3>
<p>CDP è una estensione del modello di programmazione CUDA che permette la creazione e sincronizzazione dinamica (a runtime) di nuovi kernel direttamente dalla GPU.</p>
<ul>
<li>Molto utile per implementare algoritmo ricorsivi sulla GPU.</li>
<li>È possibile posticipare a runtime la decisione su quanti blocchi e griglie creare sul device (vedi raffinamento adattivo della griglia nella simulazione di fluido dinamica)</li>
<li>Elimina in alcuni casi continui trasferimenti di memoria tra CPU e GPU</li>
<li>La GPU non è più un coprocessore totalmente governato dalla CPU ma diventa indipendente!</li>
</ul>
<p>Come funziona:</p>
<ul>
<li>Il kernel/griglia parent continua immediatamente dopo il lancio del kernel child (asincronicità).</li>
<li>Attenzione a sincronizzare accessi ad aree di memoria comuni tra parent e child. La memoria è coerente tra i due solo:
<ul>
<li>All'avvio della griglia child: il child vede tutto quello che ha fatto il padre prima del suo lancio</li>
<li>Quando la griglia child completa: il padre vede tutto quello ha fatto il figlio dopo che si è sincronizzato</li>
</ul>
</li>
<li>Puntatori a Memoria locale e smem l'uno dell'altro non sono accessibili in quanto rappresentano memoria privata</li>
<li>Un parent si considera completato solo quando tutte le griglie child create dai suoi thread (tutti) hanno terminato l'esecuzione (sincronizzazione implicita se il padre termina prima dei child).
<ul>
<li>Sincronizzazione esplicita possibile con <em>CudaDeviceSynchronize()</em></li>
</ul>
</li>
</ul>
<h2 id="modello-di-memoria-cuda">Modello di memoria CUDA</h2>
<h3 id="32-parlami-di-kernel-compute-bound-e-kernel-memory-bound-come-mai-%C3%A8-importante-distinguere-queste-due-categorie">32. Parlami di kernel compute bound e kernel memory bound, come mai è importante distinguere queste due categorie?**</h3>
<p>Un kernel è memory bound quando il tempo di esecuzione è limitato dalla velocità di accesso alla memoria piuttosto che dalla capacità di elaborazione dei core.</p>
<ul>
<li>Le unità di calcolo della GPU trascorrono più tempo in attesa dei dati rispetto a eseguire calcoli</li>
<li>poche operazioni per byte letto/scritto.</li>
<li>Accessi frequenti alla memoria.</li>
<li>Banda di memoria insufficiente rispetto ai requisiti del kernel.
<strong>NB</strong>: è notevole il fatto che la peak performance teorica di una GPU considerando solo le unità di calcolo sia molto maggiore rispetto alla peak performance considerando la sua bandwidth. Questo significa che una GPU è intrinsecamente limitata da quanto velocemente può alimentare le sue unità di calcolo con i dati (collo di bottiglia nel caso di workload memory-bound).</li>
</ul>
<p>Un kernel è compute Bound quando il tempo di esecuzione è limitato dalla capacità di calcolo della GPU, con sufficiente larghezza di banda per i dati.</p>
<ul>
<li>La GPU trascorre più tempo a eseguire calcoli rispetto all’attesa dei dati.</li>
<li>Operazioni aritmetiche intensive</li>
<li>Molte operazioni per byte letto/scritto</li>
</ul>
<p>Questa distinzione è utile in quanto per ottimizzare un kernel è cruciale comprendere se il collo di bottiglia risiede negli accessi alla memoria o nella capacità computazionale della GPU. Questa distinzione determina le strategie di ottimizzazione da adottare. Ad esempio:</p>
<ul>
<li>In un contesto memory bound, è di notevole importanza ottimizzare gli accessi alla memoria considerando quale sia il tipo di memoria più opportuna da usare e i pattern di accesso. Questo massimizza la bandwith effettiva di trasferimento dei dati
<ul>
<li>Distinguiamo tra:
<ul>
<li>bandwidth teorica (considera solo i dati &quot;lordi&quot; trasferiti)</li>
<li>bandwidth effettiva (byte effettivamente letti/scritti; considera gli sprechi)</li>
</ul>
</li>
</ul>
</li>
<li>In un contesto compute bound, è di notevole importanza massimizzare l'occupancy delle unità di eleaborazione.</li>
<li>In entrambi i casi la scelta del tipo di dato influisce sulle performance
<ul>
<li>memory bound: un tipo più piccolo mi permette di trasferire meno dati</li>
<li>compute bound: tipi di dato diversi possono avere un numero diverso di unità di elaborazione</li>
</ul>
</li>
</ul>
<h3 id="33-come-possiamo-capire-se-un-kernel-%C3%A8-memory-bound-o-compute-bound-che-cos%C3%A8-il-diagramma-di-roofline">33. Come possiamo capire se un kernel è memory bound o compute bound? Che cos'è il diagramma di roofline?</h3>
<p>Innanzitutto un kernel memory bound su una GPU potrebbe diventare compute bound su di un altra e viceversa. Questo perchè GPU diverse hanno bandwidth e velocità di elaborazione diverse. Per stabilire la tipologia di un kernel utilizziamo quindi due metriche:</p>
<ul>
<li>Intensità aritmetica
<ul>
<li>dipende solo dal kernel</li>
<li>definita come il rapporto tra la quantità di operazioni di calcolo e il volume di dati trasferiti dalla/verso la memoria di un kernel: AI = FLOPs / Byte richiesti</li>
<li>misura quante operazioni il kernel fa per byte trasferito</li>
</ul>
</li>
<li>Soglia di intensità aritmetica
<ul>
<li>dipende dalla GPU in considerazione</li>
<li>definita come il rapporto tra la peak performance per una determinata operazione e la bandwidth massima (teorica) della GPU Soglia(AI) = Theoretical Computational Peak Performance (FLOPs/s) / Bandwidth Peak Performance (Bytes/s)</li>
<li>misura quante operazioni la GPU è in grado di eseguire per byte trasferito</li>
</ul>
</li>
</ul>
<p>Unendo le informazioni di queste due metriche possiamo definire un kernel come:</p>
<ul>
<li>memory bound: se AI &lt; Soglia(AI)
<ul>
<li>ovvero il kernel ha bisogno di eseguire meno operazioni per byte trasferito rispetto a quante ne supporta la GPU</li>
</ul>
</li>
<li>compute bound: se AI &gt; soglia(AI)
<ul>
<li>il contrario di sopra</li>
</ul>
</li>
</ul>
<p>Il diagramma di roofline è solo una visualizzazione grafica di quanto detto sopra:</p>
<ul>
<li>la soglia(AI) è il punto in cui la linea diventa parallela all'asse x</li>
<li>in base alla AI, il kernel si può collocare prima o dopo il punto di svolta definito dalla soglia(AI) rendendo immediatamente visibile se esso è memory bound o compute bound</li>
<li>abbiamo roofline diverse per tipi di dato e memorie diverse</li>
</ul>
<h3 id="34-che-tipi-di-memoria-esistono-in-cuda">34. Che tipi di memoria esistono in CUDA?</h3>
<ul>
<li>
<p><strong>Registri</strong>:</p>
<ul>
<li>Ci vanno dentro le variabili locali all'interno dei kernel</li>
<li>Strettamente privati per thread con durata limitata all'esecuzione del kernel.</li>
<li>Allocati dinamicamente tra warp attivi in un SM; Minor uso di registri per thread permette di avere più <strong>blocchi</strong> concorrenti per SM (maggiore occupancy).</li>
<li>Limite di [63-255] registri per thread</li>
<li>Register Spilling verso la memoria locale</li>
</ul>
</li>
<li>
<p><strong>Memoria locale</strong>:</p>
<ul>
<li>off-chip (DRAM)</li>
<li>Privata per thread</li>
<li>Utilizzata per variabili che non possono essere allocate nei registri a causa di limiti di spazio (array locali, grandi strutture)</li>
<li>Variabili che eccedono il limite di registri del kernel finiscono qua (register spill)</li>
</ul>
</li>
<li>
<p><strong>SMEM e Cache L1</strong>:</p>
<ul>
<li>Ogni SM ha memoria on-chip (molto veloce) limitata (es. 48-228 KB), condivisa tra smem e cache L1</li>
<li>SMEM praticamente una cache programmabile condivisa tra thread di un blocco; Cache L1 Serve tutti i thread dell'SM</li>
<li>SMEM richiede sincronizzazione per prevenire corse critiche</li>
<li>La quantità da assegnare alla smem rispetto alla cache L1 è configurabile</li>
</ul>
</li>
<li>
<p><strong>Memoria costante</strong>:</p>
<ul>
<li>off-chip (DRAM)</li>
<li>scope globale (visibile a tutti i kernel)</li>
<li>Inizializzata dall'host e read-only per i kernel</li>
</ul>
</li>
<li>
<p><strong>Memoria Texture</strong>:</p>
<ul>
<li>...</li>
</ul>
</li>
<li>
<p><strong>Memoria Globale</strong>:</p>
<ul>
<li>Memoria più grande e pià lente, e più comunemente usata sulla GPU.</li>
<li>Memoria principale off-chip (DRAM) della GPU, accessibile tramite transazioni da 32, 64, o 128 byte.</li>
<li>Scope e lifetime globale (da qui global memory)
<ul>
<li>Accessibile da ogni thread di tutti i kernel</li>
<li>Occhio alle corse critiche ed alla sincronizzazione (no sincronizzazione tra blocchi)</li>
</ul>
</li>
<li>Fattori chiave per l'efficienza:
<ul>
<li>Coalescenza: Raggruppare accessi di thread adiacenti a indirizzi contigui.</li>
<li>Allineamento: Indirizzi di memoria allineati con dim delle transazioni</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Vari tipi di cache</strong>:</p>
<ul>
<li>Cache L1
<ul>
<li>Ogni SM (non SMSP) ne ha una propria</li>
</ul>
</li>
<li>Cache L2
<ul>
<li>Unica e condivisa tra SM.</li>
<li>Funge da ponte tra le cache L1 più veloci e la memoria principale più lenta.</li>
</ul>
</li>
<li>Constant Cache (sola lettura, per SM)</li>
<li>Texture Cache (sola lettura, per SM)</li>
</ul>
</li>
</ul>
<p>Le cache GPU, come quelle CPU, sono memorie on chip <strong>non programmabili</strong> utilizzate per memorizzare temporaneamente porzioni della memoria principale per accessi più veloci.</p>
<h3 id="35-come-vengono-trasferiti-i-dati-dalla-memoria-dellhost-alla-memoria-del-device-a-che-cosa-bisogna-stare-attenti-in-questo-processo">35. Come vengono trasferiti i dati dalla memoria dell'host alla memoria del device? A che cosa bisogna stare attenti in questo processo?</h3>
<p>I dati nell'host vengono trasferiti sul device tramite il bus PCIe. Questo bus ha una bandwidth notevolemente minore rispetto a quella della memoria del device e potrebbe essere quindi un collo di bottiglia nell'esecuzione dell'applicazione. Diventa essenziale quindi massimizzare la velocità di trasferimento dati tra host e device; per fare questo bisogna prima capire come vengono effettivamente trasferiti i dati su i due dispositivi.</p>
<p>Memoria Pageable:</p>
<ul>
<li>La memoria allocata dall’host di default è pageable (soggetta a swap-out notificati tramite page fault).</li>
<li>La GPU non può accedere in modo sicuro alla memoria host pageable (mancanza di controllo sui page fault).</li>
</ul>
<p>Come avviene allora il trasferimento da Memoria Pageable?</p>
<ul>
<li>Il driver CUDA alloca temporaneamente memoria host pinned (non soggetta a swap-out, bloccata in RAM).</li>
<li>Copia i dati dalla memoria host sorgente alla memoria pinned.</li>
<li>Trasferisce i dati dalla memoria pinned alla memoria del device (in modo sicuro siccome c'è la garanzia che i dati siano effettivamente presenti in RAM).</li>
</ul>
<p><strong>NB</strong>: L'overhead dovuto alla allocazione temporanea di memoria pinned e copia dei dati su quest'ultima è la causa del peggioramento di performance di quando si fanno trasferimenti multipli rispetto a un trasferimento grande (Con un singolo trasferimento pago il costo di allocazione e copia 1 volta, con <em>n</em> trasferimenti lo pago <em>n</em> volte).</p>
<p>Cio a cui bisogna fare attenzione è quindi evitare trasferimenti multipli quando non necessario. In alternativa posso, allocare direttamente della memoria pinned sull'host e, non solo non mi devo preoccupare di raggrupare i trasferimenti, ma evito anche proprio il costo di copia che avevamo prima da memoria paginabile a pinned (inoltre, la memoria pinned mi permette anche di effettuare dei trasferimenti asincroni). La memoria pinned però è più costosa da allocare e se ne alloco troppa possono degradare le prestazioni dell'host.</p>
<h3 id="36-che-cos%C3%A8-la-memoria-zero-copy">36. Che cos'è la memoria zero copy?</h3>
<p>La memoria &quot;Zero-Copy&quot; è una tecnica che consente al device di accedere direttamente alla memoria dell'host senza la necessità di copiare esplicitamente i dati tra le due memorie. In pratica è memoria pinned dell’host che è <strong>mappata nello spazio degli indirizzi del device</strong> e di conseguenza è accessibile a quest'ultimo (stessa memoria fisica ma puntatori comunque diversi).</p>
<ul>
<li>È un'eccezione alla regola che l'host non può accedere direttamente alle variabili del dispositivo e viceversa.</li>
<li>Utile per dati a cui si accede raramente, evitando copie in memoria device e riducendo l'occupazione di quest'ultima
<ul>
<li>Evita trasferimenti espliciti (impliciti per il PCIe)</li>
</ul>
</li>
<li>Peggiora le prestazioni se utilizzata per operazioni di lettura/scrittura frequenti o con grandi blocchi di dati in quanto ogni transazione alla memoria mappata passa per il bus PCIe che ha una bandwidth piccola.</li>
<li>Inoltre, si ha accesso concorrente a un area di memoria comune da parte di host e device... Necessità di sincronizzazione altrimenti corse critiche</li>
</ul>
<h3 id="37-che-cosa-sono-unified-virtual-addressing-uva-e-unified-memory-um">37. Che cosa sono Unified Virtual Addressing (UVA) e Unified Memory (UM)?</h3>
<p>UVA è una tecnica che permette alla CPU e alla GPU di condividere lo stesso spazio di indirizzamento <strong>virtuale</strong> (la memoria fisica rimane distinta). Elimina quindi la distinzione tra un puntatore (adesso virtuale) host e uno device.</p>
<p>NB: Con UVA però, si ha comunque bisogno di sapere se si sta allocando memoria su GPU o CPU dato che questa tecnica <strong>non gestisce la migrazione dei dati</strong> nelle corrette memoria fisiche, richiedendo trasferimenti manuali espliciti.</p>
<p>UM è una estensione di UVA che oltre allo spazio di indirizzamento unico gestisce in maniera automatica anche i trasferimenti di memoria tra i vari dispositivi. Si parla di <em>managed memory</em> (essa è specificabile con opportune keyword).</p>
<ul>
<li>Non è più necessario esplicitare i trasferimenti (come con la memoria zero copy) e non è più necessario distinguere tra puntatori host e device (come con UVA). In pratica posso fare <em>cudaMallocManaged/malloc</em> e non preoccuparmi più di niente come se stessi utilizzando un unico dispositivo.</li>
</ul>
<p>Queste due tecniche astraggono i dettagli di gestione della memoria tra i vari dispositivi eliminando la necessità di duplicare i puntatori, fare trasferimenti, ecc. Come ogni astrazione però, peggiora la performance dato che il sistema deve capire dove e come trasferire i dati ed inoltre il posizionamento dei dati potrebbe non essere ottimale. Per le performance massime, la gestione &quot;classica&quot; è migliore.</p>
<h3 id="38-che-cosa-si-intende-con-pattern-di-accesso-alla-memoria-come-mai-%C3%A8-importante-avere-un-pattern-ottimo-per-le-performance-di-un-kernel">38. Che cosa si intende con pattern di accesso alla memoria? Come mai è importante avere un pattern ottimo per le performance di un kernel?</h3>
<p>Facciamo un passo indietro e definiamo prima che cos'è una transazione di memoria. Abbiamo che:</p>
<ul>
<li>Le operazioni di memoria sono emesse ed eseguite per warp (32 thread).</li>
<li>Ogni thread fornisce l'indirizzo di memoria a cui vuole accedere, e la dimensione della richiesta del warp dipende dal tipo di dato (es. 32x4B per int, 32x8B per double).</li>
<li>La richiesta (lettura o scrittura) è servita da una o più transazioni di memoria.</li>
<li>Quest'ultime sono delle operazioni atomiche di lettura/scrittura tra la memoria globale e gli SM della GPU.</li>
<li>Le transazioni di memoria avvengono in blocchi di dimensioni variabili, come 128 byte o 32 byte</li>
</ul>
<p>Con i pattern di accesso alla memoria si classifica come sono distribuiti gli indirizzi di una richiesta di accesso alla memoria globale da parte dei thread di un warp. In particolare si distinguono due caratteristiche:</p>
<ol>
<li>Allineamento: quando l'indirizzo iniziale della transazione è multiplo della dimensione di quest'ultima.</li>
<li>Coalescenza: quando tutti i 32 thread di un warp accedono ad un blocco contiguo di memoria.
Entrambe queste caratteristiche sono desiderabili per ridurre al minimo il numero di transazioni richieste per servire la memoria richiesta dal warp. Quando una di queste due proprietà è assente (non sempre è possibile averle entrambe) il numero di transazioni diventa maggiore del minimo teorico e, siccome esse corrispondono ad accessi alla memoria aggiuntivi sequenziale, si ha un peggioramento delle performance.</li>
</ol>
<p>Con un pattern di accesso ottimale si riesce a massimizzare la bandwidth effettiva nella lettura/scrittura dei dati, e siccome la maggior parte delle applicazioni GPU è limitata dalla larghezza di banda della memoria DRAM, ottimizzare l'uso della memoria globale è fondamentale per le prestazioni del kernel.</p>
<h3 id="39-puoi-farmi-qualche-esempio-di-utilizzo-della-smem">39. Puoi farmi qualche esempio di utilizzo della SMEM</h3>
<ol>
<li>Come ogni forma di memoria condivisa può essere utilizzata come canale di comunicazione per i thread appartenenti allo stesso blocco.</li>
<li>Memoria scratch pad temporanea per elaborare dati on-chip e <strong>migliorare i pattern di accesso alla global memory</strong>. Ad esempio, nel caso della trasposta di una matrice, piuttosto che leggere direttamente le colonne in maniera non coalescente, posso prima far caricare a tutti i thread (del blocco, siamo limitati in dimensione ma posso gestire con i tile) i dati della matrice in smem <strong>in maniera coalescente</strong>, e poi accedere alla smem in maniera NON coalescente, siccome però stiamo accedendo alla smem e non alla memoria globale, alla peggio ci saranno dei bank conflict.</li>
<li>Memoria scratch pad temporanea per limitare le sincronizzazioni dovute alle operazioni atomiche come nel caso dell'istogramma. Se preparo una copia dei dati in smem per ogni blocco e faccio lavorare i thread sulla smem piuttosto che sulla memoria globale, limito di molto la concorrenza nell'accesso ai dati condivisi e quindi le relative sincronizzazioni.</li>
<li>Come memoria temporanea in cui carico i dati per evitare accessi multipli alla memoria globale come nel caso della riduzione parallela. Se ogni thread carica un dato in smem, i thread che prima facevano accessi multipli alla memoria globale ora ne fanno solo uno (per caricare il proprio dato), e gli accessi multipli si spostano verso la smem.</li>
</ol>
<h3 id="40-come-si-utilizza-la-smem">40. Come si utilizza la SMEM?</h3>
<ol>
<li>Caricamento in Shared Memory (Global → Shared)</li>
<li>Sincronizzazione Post-Caricamento
<ul>
<li>in modo che tutti i thread vedano nella smem i dati caricati dagli altri</li>
</ul>
</li>
<li>Elaborazione Dati</li>
<li>(Opzionale) Sincronizzazione Post-Elaborazione
<ul>
<li>in modo che tutti i thread vedano nella smem i dati caricati dagli altri</li>
<li>opzionale se non mi serve vedere le modifiche degli altri thread</li>
</ul>
</li>
<li>Scrittura dei Risultati (Shared → Global)</li>
</ol>
<h3 id="41-che-cos%C3%A8-un-bank-conflict">41. Che cos'è un bank conflict?</h3>
<p>La shared memory è suddivisa in 32 banchi (warp size) di memoria, ovvero 32 bin contenenti una word. Questa suddivisione permette ai thread di uno stesso warp di accedere contemporaneamente alla smem <strong>se i thread accedono ad indirizzi di quest'ultima corrispondenti a banchi diversi</strong>.</p>
<p>Quando più tread tentano di accedere ad <strong>indirizzi diversi corrispondenti allo stesso banco</strong> si verifica un bank conflict che ha come conseguenza la sequenzializzazione delle risposte ai vari thread. Questo è chiaramente non desiderabile in quando diminuisce la potenziale bandwidth.</p>
<h2 id="domande-reali">DOMANDE REALI</h2>
<h3 id="differenza-tra-simt-e-simd">Differenza tra SIMT e SIMD</h3>
<h3 id="warp-scheduler-e-dispatcher">Warp scheduler e dispatcher</h3>
<h3 id="gigatrhread-engine-e-occupancy">gigatrhread engine e occupancy</h3>
<h3 id="shared-memory-come-accedere-e-come-allocare">Shared Memory, come accedere e come allocare</h3>
<h3 id="occupancy-definizione-teorica-vs-effettiva">Occupancy (Definizione, Teorica vs Effettiva)</h3>
<h3 id="zero-copy-memory">Zero-Copy Memory</h3>
<h3 id="shared-memory-struttura">Shared Memory struttura</h3>
<h3 id="shared-memory-vs-cache-l1">Shared Memory vs Cache L1</h3>
<h3 id="unified-memory">Unified Memory</h3>
<h3 id="its">ITS</h3>
<h3 id="metodi-per-trasferire-memoria-da-host-a-device">Metodi per trasferire memoria da host a device</h3>

</body>
</html>
